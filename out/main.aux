\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand*\new@tpo@label[2]{}
\abx@aux@refcontext{nyt/global//global/global}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\babel@aux{english}{}
\HyPL@Entry{2<</S/D /St 2>>}
\HyPL@Entry{3<</P()>>}
\HyPL@Entry{4<</S/R>>}
\@writefile{toc}{\contentsline {chapter}{\nonumberline List of Figures}{II}{chapter*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\nonumberline List of Tables}{III}{chapter*.4}\protected@file@percent }
\AC@undonewlabel{acro:bbgc}
\newlabel{acro:bbgc}{{}{III}{\listtablename }{section*.5}{}}
\acronymused{bbgc}
\acronymused{bbgc}
\@writefile{toc}{\contentsline {chapter}{List of Acronyms}{IV}{chapter*.6}\protected@file@percent }
\newacro{bbgc}[\AC@hyperlink{bbgc}{bbgc}]{bert-base-german-cased}
\newacro{bert}[\AC@hyperlink{bert}{BERT}]{Bidirectional Encoders from Transformers}
\newacro{bpe}[\AC@hyperlink{bpe}{BPE}]{Byte Pair Encoding}
\newacro{cl}[\AC@hyperlink{cl}{CL}]{Computational Linguistics}
\newacro{gerparcor}[\AC@hyperlink{gerparcor}{GerParCor}]{German Parliamentary Corpus}
\newacro{hanta}[\AC@hyperlink{hanta}{HanTa}]{Hanover Tagger}
\newacro{lm}[\AC@hyperlink{lm}{LM}]{Language Model}
\newacro{lstm}[\AC@hyperlink{lstm}{LSTM}]{Long Short-Term Memory}
\newacro{ml}[\AC@hyperlink{ml}{ML}]{Machine Learning}
\newacro{mlm}[\AC@hyperlink{mlm}{MLM}]{Masked Language Model}
\newacro{nlp}[\AC@hyperlink{nlp}{NLP}]{Natural Language Processing}
\newacro{pos}[\AC@hyperlink{pos}{POS}]{Part of Speech}
\HyPL@Entry{8<</S/D>>}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:introduction}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Overview}{2}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:overview}{{2}{2}{Overview}{chapter.2}{}}
\abx@aux@cite{0}{ATTENTION}
\abx@aux@segm{0}{0}{ATTENTION}
\abx@aux@cite{0}{BERTHIGH1}
\abx@aux@segm{0}{0}{BERTHIGH1}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{3}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:methodology}{{3}{3}{Methodology}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Requirements}{3}{section.3.1}\protected@file@percent }
\newlabel{sec:requirements}{{1}{3}{Requirements}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Pipeline Structure \& Transformers Library}{3}{subsection.3.1.1}\protected@file@percent }
\newlabel{subsec:architecture}{{1.1}{3}{Pipeline Structure \& Transformers Library}{subsection.3.1.1}{}}
\AC@undonewlabel{acro:bert}
\newlabel{acro:bert}{{1.1}{3}{Pipeline Structure \& Transformers Library}{section*.7}{}}
\acronymused{bert}
\AC@undonewlabel{acro:nlp}
\newlabel{acro:nlp}{{1.1}{3}{Pipeline Structure \& Transformers Library}{section*.8}{}}
\acronymused{nlp}
\abx@aux@page{1}{3}
\AC@undonewlabel{acro:lstm}
\newlabel{acro:lstm}{{1.1}{3}{Pipeline Structure \& Transformers Library}{section*.9}{}}
\acronymused{lstm}
\acronymused{bert}
\AC@undonewlabel{acro:lm}
\newlabel{acro:lm}{{1.1}{3}{Pipeline Structure \& Transformers Library}{section*.10}{}}
\acronymused{lm}
\acronymused{bert}
\AC@undonewlabel{acro:mlm}
\newlabel{acro:mlm}{{1.1}{3}{Pipeline Structure \& Transformers Library}{section*.11}{}}
\acronymused{mlm}
\acronymused{nlp}
\acronymused{bert}
\acronymused{nlp}
\acronymused{bert}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic integration of Wordmap segmentation into the BERT architecture. Continuous lines show non-selective channels, while dotted lines only pass data selectively}}{4}{figure.3.1}\protected@file@percent }
\newlabel{fig:pipeline}{{1}{4}{Schematic integration of Wordmap segmentation into the BERT architecture. Continuous lines show non-selective channels, while dotted lines only pass data selectively}{figure.3.1}{}}
\AC@undonewlabel{acro:pos}
\newlabel{acro:pos}{{1.1}{4}{Pipeline Structure \& Transformers Library}{section*.12}{}}
\acronymused{pos}
\abx@aux@cite{0}{oscar}
\abx@aux@segm{0}{0}{oscar}
\abx@aux@cite{0}{FLOTA}
\abx@aux@segm{0}{0}{FLOTA}
\abx@aux@cite{0}{GERPARCOR}
\abx@aux@segm{0}{0}{GERPARCOR}
\abx@aux@cite{0}{oscarsub2}
\abx@aux@segm{0}{0}{oscarsub2}
\abx@aux@cite{0}{oscarsub1}
\abx@aux@segm{0}{0}{oscarsub1}
\abx@aux@cite{0}{wiktionary}
\abx@aux@segm{0}{0}{wiktionary}
\abx@aux@cite{0}{hanta}
\abx@aux@segm{0}{0}{hanta}
\abx@aux@cite{0}{olmpics}
\abx@aux@segm{0}{0}{olmpics}
\AC@undonewlabel{acro:hanta}
\newlabel{acro:hanta}{{1.1}{5}{Pipeline Structure \& Transformers Library}{section*.13}{}}
\acronymused{hanta}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Data}{5}{subsection.3.1.2}\protected@file@percent }
\newlabel{subsec:data}{{1.2}{5}{Data}{subsection.3.1.2}{}}
\AC@undonewlabel{acro:gerparcor}
\newlabel{acro:gerparcor}{{1.2}{5}{Data}{section*.14}{}}
\acronymused{gerparcor}
\abx@aux@page{2}{5}
\abx@aux@page{3}{5}
\acronymused{gerparcor}
\acronymused{gerparcor}
\acronymused{gerparcor}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Benchmark}{5}{subsection.3.1.3}\protected@file@percent }
\newlabel{subsec:benchmark}{{1.3}{5}{Benchmark}{subsection.3.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Implementation}{5}{section.3.2}\protected@file@percent }
\newlabel{sec:implementation}{{2}{5}{Implementation}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Tokenizer}{5}{subsection.3.2.1}\protected@file@percent }
\newlabel{subsec:tokenizer}{{2.1}{5}{Tokenizer}{subsection.3.2.1}{}}
\abx@aux@cite{0}{hanta}
\abx@aux@segm{0}{0}{hanta}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Generating a custom pre-training vocabulary}{6}{subsubsection*.16}\protected@file@percent }
\newlabel{subsubsec:generating-a-custom-pre-training-vocabulary}{{2.1}{6}{Generating a custom pre-training vocabulary}{subsubsection*.16}{}}
\acronymused{gerparcor}
\acronymused{hanta}
\acronymused{pos}
\acronymused{pos}
\acronymused{pos}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Wordmap generation}}{7}{algorithm.1}\protected@file@percent }
\newlabel{alg:wordmap}{{1}{7}{Generating a custom pre-training vocabulary}{algorithm.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Example wordmaps for $target = \texttt  {verstehen}$}}{8}{table.3.1}\protected@file@percent }
\newlabel{tab:wordmaps}{{1}{8}{Example wordmaps for $target = \texttt {verstehen}$}{table.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline BERT Tokenizer Modification}{8}{subsubsection*.18}\protected@file@percent }
\newlabel{subsubsec:tokenizer-modification}{{2.1}{8}{BERT Tokenizer Modification}{subsubsection*.18}{}}
\AC@undonewlabel{acro:POS}
\newlabel{acro:POS}{{2.1}{8}{BERT Tokenizer Modification}{section*.19}{}}
\acronymused{POS}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Target Segmentation}}{9}{algorithm.2}\protected@file@percent }
\newlabel{alg:segmenter}{{2}{9}{BERT Tokenizer Modification}{algorithm.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Permutations \textsc  {segmenter} generates from target i.e. \texttt  {verstehen}. Dotted segments mark the segmentation chosen later by the maximization function during tokenization.}}{10}{figure.3.2}\protected@file@percent }
\newlabel{fig:segmentationtree}{{2}{10}{Permutations \textsc {segmenter} generates from target i.e. \texttt {verstehen}. Dotted segments mark the segmentation chosen later by the maximization function during tokenization}{figure.3.2}{}}
\newlabel{eq:maximization}{{3.1}{10}{BERT Tokenizer Modification}{equation.3.2.1}{}}
\abx@aux@cite{0}{METZLER2016}
\abx@aux@segm{0}{0}{METZLER2016}
\abx@aux@cite{0}{bertbasegermancased}
\abx@aux@segm{0}{0}{bertbasegermancased}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces ID references assigned to full model names}}{11}{table.3.2}\protected@file@percent }
\newlabel{tab:modelids}{{2}{11}{ID references assigned to full model names}{table.3.2}{}}
\acronymused{pos}
\abx@aux@page{4}{11}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Masked Language Model}{11}{subsection.3.2.2}\protected@file@percent }
\newlabel{subsec:masked-language-model}{{2.2}{11}{Masked Language Model}{subsection.3.2.2}{}}
\acronymused{bbgc}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Benchmark}{11}{subsection.3.2.3}\protected@file@percent }
\newlabel{subsec:olmpics-benchmark}{{2.3}{11}{Benchmark}{subsection.3.2.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces List of all used models and their hyperparameters. LR = learning rate, WU = warmup steps.}}{12}{table.3.3}\protected@file@percent }
\newlabel{tab:modelshyper}{{3}{12}{List of all used models and their hyperparameters. LR = learning rate, WU = warmup steps}{table.3.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{13}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:results}{{4}{13}{Results}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Benchmark}{13}{section.4.1}\protected@file@percent }
\newlabel{sec:benchmark}{{1}{13}{Benchmark}{section.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Tokenization}{13}{section.4.2}\protected@file@percent }
\newlabel{sec:tokenization}{{2}{13}{Tokenization}{section.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Metrics for masked language model trained on the Oscar dataset with Wordmap infused tokenization. Evaluated on sequence classification task.}}{13}{table.4.4}\protected@file@percent }
\newlabel{tab:mlm-wmt-oscar500k}{{4}{13}{Metrics for masked language model trained on the Oscar dataset with Wordmap infused tokenization. Evaluated on sequence classification task}{table.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Metrics for masked language model trained on the GerParCor dataset with Wordmap infused tokenization. Evaluated on sequence classification task.}}{14}{table.4.5}\protected@file@percent }
\newlabel{tab:mlm-wmt-gpc500k}{{5}{14}{Metrics for masked language model trained on the GerParCor dataset with Wordmap infused tokenization. Evaluated on sequence classification task}{table.4.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Metrics for masked language model trained on the GerParCor dataset with \ac {bbgc} tokenization. Evaluated on sequence classification task.}}{14}{table.4.6}\protected@file@percent }
\acronymused{bbgc}
\newlabel{tab:mlm-std-oscar500k}{{6}{14}{Metrics for masked language model trained on the GerParCor dataset with \ac {bbgc} tokenization. Evaluated on sequence classification task}{table.4.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Metrics for masked language model trained on the GerParCor dataset with \ac {bbgc} tokenization. Evaluated on sequence classification task.}}{14}{table.4.7}\protected@file@percent }
\acronymused{bbgc}
\newlabel{tab:mlm-std-gpc500k}{{7}{14}{Metrics for masked language model trained on the GerParCor dataset with \ac {bbgc} tokenization. Evaluated on sequence classification task}{table.4.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Metrics for masked language model baseline bert-base-german-cased\let \reserved@d =[\def \par }}{14}{table.4.8}\protected@file@percent }
\newlabel{tab:bert-base-german-cased}{{8}{14}{Metrics for masked language model baseline bert-base-german-cased\footnote {\href {https://huggingface.co/bert-base-german-cased}{bert-base-german-cased}}. Evaluated on sequence classification task}{table.4.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Test score summary for all evaluated models.}}{14}{table.4.9}\protected@file@percent }
\newlabel{tab:test-summary}{{9}{14}{Test score summary for all evaluated models}{table.4.9}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion}{15}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:discussion}{{5}{15}{Discussion}{chapter.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{16}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:conclusion}{{6}{16}{Conclusion}{chapter.6}{}}
\abx@aux@cite{0}{GERPARCOR}
\abx@aux@segm{0}{0}{GERPARCOR}
\abx@aux@cite{0}{DEEPL}
\abx@aux@segm{0}{0}{DEEPL}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Testchapter}{17}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1}Citing}{17}{section.7.1}\protected@file@percent }
\abx@aux@page{5}{17}
\@writefile{toc}{\contentsline {section}{\numberline {2}Quoting}{17}{section.7.2}\protected@file@percent }
\abx@aux@page{6}{17}
\@writefile{toc}{\contentsline {section}{\numberline {3}Referencing}{17}{section.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\nonumberline Bibliography}{18}{chapter*.20}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\abx@aux@page{7}{18}
\abx@aux@page{8}{18}
\abx@aux@page{9}{18}
\abx@aux@page{10}{18}
\abx@aux@page{11}{18}
\abx@aux@page{12}{18}
\abx@aux@page{13}{18}
\abx@aux@page{14}{18}
\abx@aux@page{15}{19}
\abx@aux@page{16}{19}
\abx@aux@page{17}{19}
\abx@aux@page{18}{19}
\abx@aux@page{19}{19}
\abx@aux@page{20}{19}
\abx@aux@page{21}{19}
\abx@aux@page{22}{19}
\abx@aux@page{23}{19}
\abx@aux@page{24}{19}
\abx@aux@page{25}{20}
\abx@aux@page{26}{20}
\abx@aux@page{27}{20}
\abx@aux@page{28}{20}
\abx@aux@page{29}{20}
\abx@aux@page{30}{20}
\abx@aux@read@bbl@mdfivesum{0BE227ECC9669D16C14816663AA658BE}
\abx@aux@read@bblrerun
\abx@aux@defaultrefcontext{0}{GERPARCOR}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{mixednoiseinterlanguage}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{LINGUISTICTYPOLOGY}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{MORPHOLOGICALCOMPLEXITY}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{bentz-etal-2016-comparison}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{INDOEUROPE}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{morpheme}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{measuresofMC}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{comrie1989}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{morfessor}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{DEEPL}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{MORPHOSYNTAXCOMPLEXITY1}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{METZLER2016}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{FLOTA}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{lehmann}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{subwordvsmorfessor}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{gpt}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{MONOLINGUAL}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{WORDPIECEOG}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{POLYSYNTHLM}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{BPE}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{TOKENIZATIONIMPACT}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{ATTENTION}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{WORDPIECEGOOGLE}{nyt/global//global/global}
\global\@namedef{scr@dte@chapter@lastmaxnumwidth}{10.96793pt}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{10.37993pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{18.59993pt}
\global\@namedef{scr@dte@table@lastmaxnumwidth}{10.37993pt}
\global\@namedef{scr@dte@figure@lastmaxnumwidth}{10.37993pt}
\@writefile{toc}{\providecommand\tocbasic@end@toc@file{}\tocbasic@end@toc@file}
\@writefile{lof}{\providecommand\tocbasic@end@toc@file{}\tocbasic@end@toc@file}
\@writefile{lot}{\providecommand\tocbasic@end@toc@file{}\tocbasic@end@toc@file}
\gdef \@abspage@last{28}
