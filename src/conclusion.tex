What was done?
How did it go?
What went wrong?
What went well?
What was learned from this?
What are future applications?

In almost all statistical modeling the goal is to model reality as precisely as feasible.
Language models are no exception.
The accuracy of a model should increase with the number of functional components of natural language being integrated into the model.
This is seen in e.g.\ the implementation of vocabularies, just one of many attempts to automatically identify meaningful units in language.
Even higher levels of language found in domains from ordinary pragmatics to scientific reasoning are sought after in language modeling.
While languages are observed to change slowly over time, sometimes dropping and adding features of their inventory, computational linguistics has to keep producing models that keep up with the reality of language.
The supervised tokenization in this thesis illustrates just a small part of the potential in tailored modeling.
