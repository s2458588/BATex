
This thesis showcases the use of specific intervention in tokenization subsystems of machine learning.
The intent of this thesis is to inject linguistic bias into the machine learning framework of BERT to sharpen the analytical capacities of a masked language model.
In this chapter the background, intentions and scope of the thesis are covered.


\section{Motivation}
\label{sec:motivation}
\uppercase{Why is this subject relevant}
There is an ongoing urge in the \acf{cl} community to understand natural language.
Research in the past decades shows use of frequentist and statistical methods (such as gpt and bert \uppercase{ZITATION}) to their advantage, leading to the emergence of the first machine learning (ML) models.
It became apparent that these ML models are the best currently available approach to an automated understanding of natural language.
The structural parallels of machine learning to human learning have often been drawn (\uppercase{zitation)}) to demonstrate how similar and more importantly: how different both can be.
A powerful feature of \ac{ml} (as opposed to human learning) is the possibility of actively controlling the the learning parameters in a supervised environment.
To test the efficiency of \uppercase{ML} parameters a variety of tasks (\uppercase{zitation}) are designed and applied.
A trained model will yield performance scores based on the quality of its training, much like humans on language tests.
But the automated modeling of language is not the first instance language modelling in a broader sense.
Traditional linguistics (\uppercase{definition} has produced fundamental research the prior to the discovery of \uppercase{ML} architectures and their implementation.
While generic ML frameworks seem appealing in the presumption that they require less work to reach somewhat satisfactoy results,
they are far from complete or perfect.
The integration of aforementioned traditional linguistic knowledge into learning processes for machine learning is the underlying motivation of this thesis.
\textbf{flota} \cite{FLOTA}
Language learners usually build up a lexicon consisting of lexemes which they will have to analyze accurately in order to be productive in that target language.
A ML model relies on a tokenizer to create such a vocabulary  (\textbf{ZITATION}).
It is programmed to segment tokens into subwords (if possible) and provide a vocabulary comprising all the components needed to analyze a given string.
Ideally those subwords will be part of the functional vocabulary in the target language, so called morphemes \textbf{ERKLÃ„RUNG}.
A morpheme is defined as the smallest unit carrying meaning in a language.
The morphemes of a language and its generated tokenizer vocabulary rarely coincide.
Typically, tokanizer vocabularies will contain a lot of noise and linguistically nonsensical segmentations or words.
Following the guiding principle that \textbf{input quality is ouput quality} not only in language learning, the morpheme vocabulary is identified as the point of leverage in the upcoming section.
Note: explain why i use tokens and words, they are interchangeable right?
holistic, need less attention to produce satisfactory
\uppercase{not just to push F, but to find a viable method of morphemic tokenization}


\section{Hypotheses}
\label{sec:hypothesis}

The following research questions will be formulated for testing:
\begin{framed}
    \begin{itemize}[itemindent=1em]
        \item[HYP1:] Adjustments to tokenization have significant impact the performance of a language model in different tasks.
    \end{itemize}
\end{framed}
How to achieve this hypothesis?
\begin{framed}
    \begin{itemize}[itemindent=1em]
        \item[HYP2:] Providing lexical information to a tokenizer increases benchmark accuracy on MLM tasks.
    \end{itemize}
\end{framed}
How to achieve this hypothesis?

\section{Scope and Structure}
\label{sec:scope-and-structure}

The following chapters are sorted into three parts.
To outline the research domain, a brief summary of the current state of morphological language modeling is given.
Next, german is described paying special attention to its morphological complexity and peer languages.
This serves as preface to the methodology, connecting characteristically matching languages to form a pool of possible  target languages.

As main part of this thesis, the methodology is layed out.
It is sectioned into a theoretical part which focuses on what implements are used and the value they hold towards lexicalizing a tokenizer

What is covered and what not?
What is the shape of this thesis and what order does it have?
