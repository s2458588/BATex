
This thesis investigates the use of specific intervention in tokenization subsystems of language modeling in machine learning.
The intent of this thesis is to inject linguistic bias into the machine learning framework of \ac{bert} to sharpen the analytical capacities of a \ac{mlm}.
This introduction includes the research motivation and intentions, the resulting hypotheses and the scope of the thesis.

\section{Motivation}
\label{sec:motivation}
There is a long-standing ambition in the \acf{cl} community to understand natural language.
Research in the past decades shows promising use of frequentist and statistical methods such as \ac{gpt} (\cite{gpt}) and \ac{bert} (\cite{ATTENTION}), leading to emergence of the powerful, problem solving \ac{ml} models.
It became apparent that these ML models are the state-of-the-art approach to an automated understanding of natural language.
The structural parallels of machine learning to human learning have often been drawn (\textcite{humanmachinelearning1}; \textcite{humanmachinelearning2}) to demonstrate their similarities and differences.
An essential feature of \ac{ml} (as opposed to human learning) is the possibility of actively controlling the learning parameters in a supervised environment.
To test the efficiency of  \ac{ml} applications, a variety of tasks (e.g.\ classification, clustering, detection) are designed and applied.
A trained model will yield performance scores based on the quality of its training, much like humans in language tests.
Yet, statistical language modeling is not the first variant of language modeling.
Traditional linguistics has accumulated extensive fundamental research prior to the discovery of \ac{ml} architectures and their implementation.
While generic \ac{ml} frameworks seem appealing in the presumption that they require less work to reach fairly acceptable results, their performance is still unsatisfactory.
The integration of aforementioned traditional linguistic knowledge into the learning processes for machine learning is the underlying motivation of this thesis.

While the research on neurophysical intake and representation of language is not entirely conclusive (\textcite[1--3]{n400}; \textcite{L2morphology}), it is assumed that language learners typically build up a lexicon consisting of lexemes (\cite[82 -- 100]{brennan2022language}), which they have to analyze accurately in order to be proficient in that target language.
A language model relies on a tokenizer to create such a vocabulary (attempts that bypass the vocabulary mechanism exist as well, e.g. \ \textcite{handsfreesegmentation}).
String-based tokenizers generally segment tokens into subwords (if possible) and provide a vocabulary comprising all the components needed to analyze a given string.
Ideally, those subwords are part of the functional vocabulary in the target language, which would make them a morpheme.
A morpheme is canonically defined as the smallest unit carrying meaning in a language (\cite{morpheme}).
Capturing lexical morphemes at the tokenization level is a problem of morphological analysis.
A standard approach implemented in the BERT architecture is WordPiece tokenization, which takes pairs of n-grams and calculates a score based on the overall frequency in the text as well as cooccurrences.
This idea was pioneered by Schuster and Nakajima (\citeyear{WORDPIECEOG}) in order to deal with the problem of near infinite vocabularies and was later adopted in a Google research project designing the final WordPiece algorithm (\cite{WORDPIECEGOOGLE}).
WordPiece is designed to find the most effective pairs of subwords in a corpus to optimize vocabulary size.
Subwords, however, are not necessarily a productive component of inventory used in inflectional morphology.
In practical use, the morphemes of a language and its generated tokenizer vocabulary rarely totally coincide.
Following a revised principle of the interaction hypothesis that - put simply - \textit{input quality is crucial for output quality} (\cite[114]{inputoutput}), the morpheme vocabulary is identified as the point of leverage in the upcoming chapter.
Underlying is the notion not to just \textquotedblleft push F scores\textquotedblright, but to explore a method of morphemic tokenization.


\section{Hypotheses}
\label{sec:hypothesis}

\centerline{The following research questions are formulated:}
\begin{framed}
    \hypertarget{hyp1}{}
    \begin{itemize}[itemindent=1em]
        \item[HYP1:] Targeting a Part-of-Speech class during tokenization affects model performance.
    \end{itemize}
\end{framed}
\noindent This is tested through designing a tokenization process with POS overhead, fine-tuning a model on that tokenization and running a sequence classification task.
\begin{framed}
    \hypertarget{hyp2}{}
    \begin{itemize}[itemindent=1em]
        \item[HYP2:] Separating lexical from functional morphemes improves segmentation in lexemic tokens.
    \end{itemize}
\end{framed}
\noindent The design for hypothesis 1 includes creating a separate POS vocabulary.
Model scores and qualitative evaluation will determine the plausibility of hypothesis 2.

\section{Scope and Structure}
\label{sec:scope-and-structure}
The upcoming \autoref{ch:overview} provides an overview of the current state of morphological tokenization to outline the research domain.
German is declared as target language, verbs being the classified tokens of interest.
The language is described paying special attention to its morphological complexity and peer languages.
This serves as preface to the methodology, connecting characteristically matching languages to form a pool of possible  target languages.

As the main part of this thesis, \autoref{ch:methodology} presents the methodology for a new tokenization method.
It is sectioned into a theoretical part and the practical implementation of the process.
The theoretical part focuses on the architecture of its components and the value they hold towards lexicalizing a tokenizer.
In the implementation, each part of the tokenization process is explained, providing algorithms and their integration into the machine learning framework.
Model training and testing are described.
Results are presented in \autoref{ch:results}, followed by a discussion of possible effects in \autoref{ch:discussion}.
The conclusion in \autoref{ch:conclusion} puts the benchmark results and tokenization method into perspective and .
%TODO: halbsÃ¤tze result und discussion-> outlook potential optimization iteration loops