What was done?
How did it go?
What went wrong?
What went well?
What was learned from this?
What are future applications?

In almost all statistical modeling the goal is to model reality as precisely as feasible.
Language models are no exception.
The accuracy of a model should increase with the number of functional components of natural language being integrated into the model.
This is seen in e.g.\ the implementation of vocabularies, just one of many attempts to automatically identify meaningful units in language.
Even higher levels of language found in domains from ordinary pragmatics to scientific reasoning are sought after in language modeling.
While languages are observed to change slowly over time, sometimes dropping and adding features of their inventory, computational linguistics has to keep producing models that keep up with the reality of language.
The supervised tokenization in this thesis illustrates just a small part of the potential in tailored modeling.



reanalysis -> lexicalization challenges this method?
lexeme detection very close to the mean in wormaps!
relative frequenz sollte evtl in die maximisierungsfunktion rein
adjustment to the maximization function: relative
control loss next time, maybe use ~\textcite{bertbasegermancasedsequel}

Method: tokenizer threshold.
Interestingly, there are many segment thresholds to choose from

there should be a filter to mitigate the natural distribution of common characters like s, n to be detected as  part of a morpheme

whole integrated system with overhead pos for all lexical categories
scaled with the degree of fusion and inflection that language has

use \ac{cl}
holistic

analog to the interaction hypothesis we have to stimulate models with selective input alongside generic input data to come closer
menschen lernen / sprechen /speichern sprache mit lexikon, und ML macht eine krücke über tokenizer. man sollte beides so nah aneinander bringen wie möglich