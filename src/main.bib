%! Author = gnom
%! Date = 08.12.22

@book{METZLER2016,
    editor={Gl{\"u}ck, Helmut and R{\"o}del, Michael},
    title={Metzler Lexikon Sprache},
    edition={5},
    publisher={J.B. Metzler},
    address={Stuttgart},
    year={2016},
    pages={141-142},
    language={ger},
    isbn={978-3-476-05486-9},
    series={Springer eBook Collection},
    doi={10.1007/978-3-476-05486-9},
    keywords={Linguistik},
    url={http://dx.doi.org/10.1007/978-3-476-05486-9},
    library={UB},
}


% TOKENIZATION INFLUENCES MODEL: VOCAB SIZE 20-40% PARAMETER SIZE
@misc{TOKENIZATIONIMPACT,
    doi = {10.48550/ARXIV.2204.08832},
    url = {https://arxiv.org/abs/2204.08832},
    author = {Toraman, Cagri and Yilmaz, Eyup Halit and Şahinuç, Furkan and Ozcelik, Oguzhan},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Impact of Tokenization on Language Models: An Analysis for Turkish},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

% INTRODUCTION OF TRANSFORMER ARCHITECTURE
@misc{ATTENTION,
    doi = {10.48550/ARXIV.1706.03762},
    url = {https://arxiv.org/abs/1706.03762},
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
    keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Attention Is All You Need},
    publisher = {arXiv},
    year = {2017},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{pytorch,
    doi = {10.48550/ARXIV.1912.01703},
    url = {https://arxiv.org/abs/1912.01703},
    author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
    keywords = {Machine Learning (cs.LG), Mathematical Software (cs.MS), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    publisher = {arXiv},
    year = {2019},
    copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{bertbasegermancasedsequel,
    title = {{G}erman{'}s Next Language Model},
    author = {Chan, Branden  and
      Schweter, Stefan  and
      M{\"o}ller, Timo},
    booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
    month = {12},
    year = {2020},
    address = {Barcelona, Spain (Online)},
    publisher = {International Committee on Computational Linguistics},
    url = {https://aclanthology.org/2020.coling-main.598},
    doi = {10.18653/v1/2020.coling-main.598},
    pages = {6788--6796},
    abstract = {In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. All trained models will be made publicly available to the research community.},
}

@misc{bertbasegermancased,
    title = {GermanBERT},
    author = {Chan, Branden  and
      Schweter, Stefan  and
      M{\"o}ller, Timo},
    month = {6},
    year = {2019},
    publisher = {Deepset AI},
    url = {https://huggingface.co/bert-base-german-cased},
}

% BERT HIGH BENCHMARK PERFORMANCE
@misc{BERTHIGH1,
    doi = {10.48550/ARXIV.1810.04805},
    url = {https://arxiv.org/abs/1810.04805},
    author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}


% DEEPL STATEMENT
@online{DEEPL,
    title={How does deepl work?},
    url={https://www.deepl.com/en/blog/how-does-deepl-work},
    journal={DeepL Translate: The world's most accurate translator},
    publisher={DeepL SE},
    author={DeepL},
    year={2021},
    month={11},
    addendum = {Last accessed: 28.12.2022},
}

@misc{MONOLINGUAL,
    doi = {10.48550/ARXIV.2012.15613},
    url = {https://arxiv.org/abs/2012.15613},
    author = {Rust, Phillip and Pfeiffer, Jonas and Vulić, Ivan and Ruder, Sebastian and Gurevych, Iryna},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models},
    publisher = {arXiv},
    year = {2020},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{measuresofMC,
    url = {https://doi.org/10.1515/lingvan-2021-0007},
    title = {What do complexity measures measure? Correlating and validating corpus-based measures of morphological complexity},
    author = {Çağrı Çöltekin and Taraka Rama},
    journal = {Linguistics Vanguard},
    doi = {doi:10.1515/lingvan-2021-0007},
    year = {2022},
    lastchecked = {2023-02-09}
}

@misc{mixednoiseinterlanguage,
    doi = {10.48550/ARXIV.2109.06772},
    url = {https://arxiv.org/abs/2109.06772},
    author = {Aepli, Noëmi and Sennrich, Rico},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7},
    title = {Improving Zero-shot Cross-lingual Transfer between Closely Related Languages by injecting Character-level Noise},
    publisher = {arXiv},
    year = {2021},
    copyright = {Creative Commons Attribution 4.0 International}
}


@inbook{comrie1989,
    place={Chicago},
    edition={2},
    title={Morphological Typology},
    booktitle={Language universals and linguistic typology},
    publisher={University of Chicago Press},
    author={Comrie, Bernard},
    year={1989},
    pages={42–56}
}

@article{INDOEUROPE,
    author = {Remco Bouckaert  and Philippe Lemey  and Michael Dunn  and Simon J. Greenhill  and Alexander V. Alekseyenko  and Alexei J. Drummond  and Russell D. Gray  and Marc A. Suchard  and Quentin D. Atkinson },
    title = {Mapping the Origins and Expansion of the Indo-European Language Family},
    journal = {Science},
    volume = {337},
    number = {6097},
    pages = {957-960},
    year = {2012},
    doi = {10.1126/science.1219669},
    URL = {https://www.science.org/doi/abs/10.1126/science.1219669},
    eprint = {https://www.science.org/doi/pdf/10.1126/science.1219669},
    abstract = {English is part of the large Indo-European language family, which includes Celtic, Germanic, Italic, Balto-Slavic, and Indo-Iranian languages. The origin of this family is hotly debated: one hypothesis places the origin north of the Caspian Sea in the Pontic steppes, from where it was disseminated by Kurgan semi-nomadic pastoralists; a second suggests that Anatolia, in modern-day Turkey, is the source, and the language radiated with the spread of agriculture. Bouckaert et al. (p. 957) used phylogenetic methods and modeling to assess the geographical spread of the Indo-European language group. The findings support the suggestion that the origin of the language family was indeed Anatolia 7 to 10 thousand years ago—contemporaneous with the spread of agriculture. Spatial models of language lineage evolution support an Anatolian homeland for Indo-European languages. There are two competing hypotheses for the origin of the Indo-European language family. The conventional view places the homeland in the Pontic steppes about 6000 years ago. An alternative hypothesis claims that the languages spread from Anatolia with the expansion of farming 8000 to 9500 years ago. We used Bayesian phylogeographic approaches, together with basic vocabulary data from 103 ancient and contemporary Indo-European languages, to explicitly model the expansion of the family and test these hypotheses. We found decisive support for an Anatolian origin over a steppe origin. Both the inferred timing and root location of the Indo-European language trees fit with an agricultural expansion from Anatolia beginning 8000 to 9500 years ago. These results highlight the critical role that phylogeographic inference can play in resolving debates about human prehistory.}}

@inproceedings{bentz-etal-2016-comparison,
    title = "A Comparison Between Morphological Complexity Measures: Typological Data vs. Language Corpora",
    author = "Bentz, Christian  and
      Ruzsics, Tatyana  and
      Koplenig, Alexander  and
      Samard{\v{z}}i{\'c}, Tanja",
    booktitle = "Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity ({CL}4{LC})",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/W16-4117",
    pages = "142--153",
    abstract = "Language complexity is an intriguing phenomenon argued to play an important role in both language learning and processing. The need to compare languages with regard to their complexity resulted in a multitude of approaches and methods, ranging from accounts targeting specific structural features to global quantification of variation more generally. In this paper, we investigate the degree to which morphological complexity measures are mutually correlated in a sample of more than 500 languages of 101 language families. We use human expert judgements from the World Atlas of Language Structures (WALS), and compare them to four quantitative measures automatically calculated from language corpora. These consist of three previously defined corpus-derived measures, which are all monolingual, and one new measure based on automatic word-alignment across pairs of languages. We find strong correlations between all the measures, illustrating that both expert judgements and automated approaches converge to similar complexity ratings, and can be used interchangeably.",
}
@ARTICLE{MORPHOSYNTAXCOMPLEXITY1,
    AUTHOR={Ehret, Katharina and Blumenthal-Dramé, Alice and Bentz, Christian and Berdicevskis, Aleksandrs},
    TITLE={Meaning and Measures: Interpreting and Evaluating Complexity Metrics},
    JOURNAL={Frontiers in Communication},
    VOLUME={6},
    YEAR={2021},
    URL={https://www.frontiersin.org/articles/10.3389/fcomm.2021.640510},
    DOI={10.3389/fcomm.2021.640510},
    ISSN={2297-900X},
    ABSTRACT={Research on language complexity has been abundant and manifold in the past two decades. Within typology, it has to a very large extent been motivated by the question of whether all languages are equally complex, and if not, which language-external factors affect the distribution of complexity across languages. To address this and other questions, a plethora of different metrics and approaches has been put forward to measure the complexity of languages and language varieties. Against this backdrop we address three major gaps in the literature by discussing statistical, theoretical, and methodological problems related to the interpretation of complexity measures. First, we explore core statistical concepts to assess the meaningfulness of measured differences and distributions in complexity based on two case studies. In other words, we assess whether observed measurements are neither random nor negligible. Second, we discuss the common mismatch between measures and their intended meaning, namely, the fact that absolute complexity measures are often used to address hypotheses on relative complexity. Third, in the absence of a gold standard for complexity metrics, we suggest that existing measures be evaluated by drawing on cognitive methods and relating them to real-world cognitive phenomena. We conclude by highlighting the theoretical and methodological implications for future complexity research.}
}



% ORIGINAL IDEA FOR WORDPIECE TOKENIZATION ALGORITHM
@INPROCEEDINGS{WORDPIECEOG,
    author={Schuster, Mike and Nakajima, Kaisuke},
    booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
    title={Japanese and Korean voice search},
    year={2012},
    volume={},
    number={},
    pages={5149-5152},
    doi={10.1109/ICASSP.2012.6289079}}

% GOOGLE INTRODUING Neural Machine Translation (NMT) and WORDPIECE (FROM WORDPIECEOG)
@misc{WORDPIECEGOOGLE,
    doi = {10.48550/ARXIV.1609.08144},
    url = {https://arxiv.org/abs/1609.08144},
    author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Łukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
    keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
    publisher = {arXiv},
    year = {2016},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

% BYTE PAIR ENCODING ORIGINAL PAPER
@inproceedings{BPE,
    title = {Neural Machine Translation of Rare Words with Subword Units},
    author = {Sennrich, Rico  and Haddow, Barry  and Birch, Alexandra},
    booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = {8},
    year = {2016},
    address = {Berlin, Germany},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/P16-1162},
    doi = {10.18653/v1/P16-1162},
    pages = {1715--1725},
}

% MORPHOLOGY: POLYSYNTHETIC LANGUAGE MODELING (CONTAINS MORPHESSOR)
@misc{POLYSYNTHLM,
    doi = {10.48550/ARXIV.2005.05477},
    url = {https://arxiv.org/abs/2005.05477},
    author = {Schwartz, Lane and Tyers, Francis and Levin, Lori and Kirov, Christo and Littell, Patrick and Lo, Chi-kiu and Prud'hommeaux, Emily and Park, Hyunji Hayley and Steimel, Kenneth and Knowles, Rebecca and Micher, Jeffrey and Strunk, Lonny and Liu, Han and Haley, Coleman and Zhang, Katherine J. and Jimmerson, Robbie and Andriyanets, Vasilisa and Muis, Aldrian Obaja and Otani, Naoki and Park, Jong Hyuk and Zhang, Zhisong},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Neural Polysynthetic Language Modelling},
    publisher = {arXiv},
    year = {2020},
    copyright = {arXiv.org perpetual, non-exclusive license},
    pages = {51--53},
}


@article{scalinglaws,
    author    = {Jared Kaplan and
               Sam McCandlish and
               Tom Henighan and
               Tom B. Brown and
               Benjamin Chess and
               Rewon Child and
               Scott Gray and
               Alec Radford and
               Jeffrey Wu and
               Dario Amodei},
    title     = {Scaling Laws for Neural Language Models},
    journal   = {CoRR},
    volume    = {abs/2001.08361},
    year      = {2020},
    url       = {https://arxiv.org/abs/2001.08361},
    eprinttype = {arXiv},
    eprint    = {2001.08361},
    timestamp = {Wed, 03 Jun 2020 10:55:13 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{fewshotTHREEpercent,
    doi = {10.48550/ARXIV.2208.03299},
    url = {https://arxiv.org/abs/2208.03299},
    author = {Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Atlas: Few-shot Learning with Retrieval Augmented Language Models},
    publisher = {arXiv},
    year = {2022},
    copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{smallmodelbigbang,
    doi = {10.48550/ARXIV.2009.07118},
    url = {https://arxiv.org/abs/2009.07118},
    author = {Schick, Timo and Schütze, Hinrich},
    keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
    publisher = {arXiv},
    year = {2020},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{adversarialmorphin,
    title = {It{'}s Morphin{'} Time! {C}ombating Linguistic Discrimination with Inflectional Perturbations},
    author = {Tan, Samson  and
      Joty, Shafiq  and
      Kan, Min-Yen  and
      Socher, Richard},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    month = jul,
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.acl-main.263},
    doi = {10.18653/v1/2020.acl-main.263},
    pages = {2920--2935},
    abstract = {Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.}
}
% Base-Inflection Encoding: FUNCTIONAL MORPHEMES BECOME TOKENS
@inproceedings{BITE,
    title = {Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding},
    author = {Tan, Samson  and
      Joty, Shafiq  and
      Varshney, Lav  and
      Kan, Min-Yen},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    month = {11},
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.emnlp-main.455},
    doi = {10.18653/v1/2020.emnlp-main.455},
    pages = {5647--5663},
    abstract = {Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English. Although comprehension by human readers is usually unimpaired by non-standard inflections, current NLP systems are not yet robust. We propose Base-Inflection Encoding (BITE), a method to tokenize English text by reducing inflected words to their base forms before reinjecting the grammatical information as special symbols. Fine-tuning pretrained NLP models for downstream tasks using our encoding defends against inflectional adversaries while maintaining performance on clean data. Models using BITE generalize better to dialects with non-standard inflections without explicit training and translation models converge faster when trained with BITE. Finally, we show that our encoding improves the vocabulary efficiency of popular data-driven subword tokenizers. Since there has been no prior work on quantitatively evaluating vocabulary efficiency, we propose metrics to do so.},
}

@misc{MORPHEMEDISCOVERYUNSUPERVISED,
    doi = {10.48550/ARXIV.CS/0205057},
    url = {https://arxiv.org/abs/cs/0205057},
    author = {Creutz, Mathias and Lagus, Krista},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7},
    title = {Unsupervised Discovery of Morphemes},
    publisher = {arXiv},
    year = {2002},
    copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004}
}



@article{IOINTERACTION,
    author ={Gass, Susan M. and Mackey, Alison},
    title ={Input, Interaction and Output: An Overview},
    journal={AILA Review},
    year ={2006},
    volume ={19},
    number ={1},
    pages ={3-17},
    doi ={https://doi.org/10.1075/aila.19.03gas},
    url ={https://www.jbe-platform.com/content/journals/10.1075/aila.19.03gas},
    publisher ={John Benjamins},
    issn ={1461-0213},
    type ={Journal Article},
    abstract ={This paper presents an overview of what has come to be known as the Interaction Hypothesis, the basic tenet of which is that through input and interaction with interlocutors, language learners have opportunities to notice differences between their own formulations of the target language and the language of their conversational partners. They also receive feedback which both modifies the linguistic input they receive and pushes them to modify their output during conversation. This paper focuses on the major constructs of this approach to SLA, namely, input, interaction, feedback and output, and discusses recent literature that addresses these issues.},
}

@article{BPEGAGE,
    author = {Gage, Philip},
    title = {A New Algorithm for Data Compression},
    year = {1994},
    issue_date = {Feb. 1994},
    publisher = {R & D Publications, Inc.},
    address = {USA},
    volume = {12},
    number = {2},
    issn = {0898-9788},
    journal = {C Users J.},
    month = {2},
    pages = {23–38},
    numpages = {16}
}

@article{LSTM,
    author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
    title = {Long Short-Term Memory},
    year = {1997},
    issue_date = {November 15, 1997},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    volume = {9},
    number = {8},
    issn = {0899-7667},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    doi = {10.1162/neco.1997.9.8.1735},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
    journal = {Neural Comput.},
    month = {11},
    pages = {1735–1780},
    numpages = {46}
}

@book{WALSGEN,
    address   = {Leipzig},
    editor    = {Matthew S. Dryer and Martin Haspelmath},
    publisher = {Max Planck Institute for Evolutionary Anthropology},
    title     = {WALS Online},
    url       = {https://wals.info/},
    year      = {2013}
}

@incollection{WALSFUS,
    address   = {Leipzig},
    author    = {Balthasar Bickel and Johanna Nichols},
    booktitle = {The World Atlas of Language Structures Online},
    editor    = {Matthew S. Dryer and Martin Haspelmath},
    publisher = {Max Planck Institute for Evolutionary Anthropology},
    title     = {Fusion of Selected Inflectional Formatives},
    url       = {https://wals.info/chapter/20},
    year      = {2013}
}

@data{germeval,
    author = {Wiegand, Michael},
    publisher = {heiDATA},
    title = {{GermEval-2018 Corpus (DE)}},
    year = {2019},
    version = {V1},
    doi = {10.11588/data/0B5VML},
    url = {https://doi.org/10.11588/data/0B5VML}
}

% DEFINITION OF MORPHEME
@misc{morpheme,
    author = {Andrew M. Colman},
    title = {morpheme},
    year = {2009},
    publisher = {Oxford University Press},
    isbn = {9780191726828},
    doi = {10.1093/acref/9780199534067.013.5219},
    url = {https://www.oxfordreference.com/view/10.1093/acref/9780199534067.001.0001/acref-9780199534067-e-5219}
}

@book{MORPHOLOGICALCOMPLEXITY,
    editor = {Baerman, Matthew and Brown, Dunstan and Corbett, Greville G.},
    title = {Morphological Complexity},
    %edition = {1},
    publisher = {Cambridge University Press},
    adress = {Cambridge},
    year = {2017},
    %language = {english},
    isbn = {1107120640},
    series = {Cambridge studies in Linguistics 153},
    doi = {10.1017/9781316343074},
    %keywords = {Morphology},
    url = {www.cambridge.org/9781107120648},
    library = {British Library}
}

% MORPHOLOGY
@inbook{FROMWORDTOPARADIGM,
    place={Cambridge},
    series={Cambridge Studies in Linguistics},
    booktitle={Morphological Typology: From Word to Paradigm},
    publisher={Cambridge University Press},
    author={Stump, Gregory and Finkel, Raphael A.},
    year={2013}, pages={iii-iv},
    collection={Cambridge Studies in Linguistics}
}

% MORPHOLOGY
@book{MMM4,
    editor = {Booij, Geert and Guevara, Emiliano and Ralli, Angeliki and Sgroi, Salvatore and Scalise, Sergio},
    year = {2003},
    month = {09},
    pages = {},
    title = {Morphology and linguistic typology. On-line Proceedings of the Fourth Mediterranean Morphology Meeting MMM4}
}

% MORPHOLOGY
@book{LINGUISTICTYPOLOGY,
    author = {Aikhenvald, Alexandra and Dixon, R},
    address={Cambridge},
    series={Cambridge Handbooks in Language and Linguistics},
    title={The Cambridge Handbook of Linguistic Typology},
    DOI={10.1017/9781316135716},
    publisher={Cambridge University Press},
    year={2017}, collection={Cambridge Handbooks in Language and Linguistics}
}

% WORD MORPHOLOGY
@article{WORDTYPOLOGY,
    author = {Dixon, R. and Aikhenvald, Alexandra},
    year = {2002},
    month = {01},
    pages = {},
    title = {Word: A cross - linguistic typology},
    isbn = {9780521818995},
    doi = {10.1017/CBO9780511486241}
}

@book{hunverbs,
    url = {https://doi.org/10.1515/9789048544608},
    title = {Syntax of Hungarian},
    title = {Postpositions and Postpositional Phrases},
    editor = {Katalin É. Kiss and Veronika Hegedus},
    publisher = {Amsterdam University Press},
    address = {Amsterdam},
    doi = {doi:10.1515/9789048544608},
    isbn = {9789048544608},
    year = {2021},
    lastchecked = {2023-02-12},
    pages = {262}
}



@article{L2morphology,
    title = {Acquisition of L2 morphology by adult language learners},
    journal = {Cortex},
    volume = {116},
    pages = {74-90},
    year = {2019},
    note = {Structure in words: the present and future of morphological processing in a multidisciplinary perspective},
    issn = {0010-9452},
    doi = {https://doi.org/10.1016/j.cortex.2019.01.012},
    url = {https://www.sciencedirect.com/science/article/pii/S0010945219300310},
    author = {Lilli Kimppa and Yury Shtyrov and Suzanne C.A. Hut and Laura Hedlund and Miika Leminen and Alina Leminen},
    keywords = {L2 acquisition, Proficiency, Morphology, Inflection, Derivation, EEG},
    abstract = {Learning a new language requires the acquisition of morphological units that enable the fluent use of words in different grammatical contexts. While accumulating research has elucidated the neural processing of native morphology, much less is known about how second-language (L2) learners acquire and process morphology in their L2. To address this question, we presented native speakers as well as beginning and advanced learners of Finnish with spoken (1) derived words, (2) inflected words, (3) novel derivations (novel combinations of existing stem + suffix), and (4) pseudo-suffixed words (existing stem + pseudo-suffix) in a passive listening EEG experiment. An early (60 msec after suffix deviation point) positive ERP response showed no difference between inflections and derivations, suggesting similar early parsing of these complex words. At 130 msec, derivations elicited a lexical ERP pattern of full-form memory-trace activation, present in the L2 beginners and advanced speakers to different degrees, implying a shift from lexical processing to more dual parsing and lexical activation of the complex forms with increasing proficiency. Pseudo-suffixed words produced a syntactic pattern in a later, 170–240 msec time-window, exhibiting enhanced ERPs compared to well-formed inflections, indicating second-pass syntactic parsing. Overall, the L2 learners demonstrated a gradual effect of proficiency towards L1-like responses. Advanced L2 learners seem to have developed memory traces for derivations and their neurolinguistic system is capable of early automatic parsing. This suggests that advanced learners have already developed sensitivity to morphological information, while such knowledge is weak in beginners. Discrepancies in ERP dynamics and topographies indicate partially differing recruitment of the language network in L1 and L2. In beginners, response differences between existing and novel morphology were scarce, implying that representations for complex forms are not yet well-established. The results suggest successful development of brain mechanisms for automatic processing of L2 morphology, capable of gradually attaining L1-like functionality with increasing proficiency.}
}


@article{n400,
    title = {Event-related potentials index lexical retrieval (N400) and integration (P600) during language comprehension},
    journal = {Brain and Cognition},
    volume = {135},
    pages = {1--3},
    year = {2019},
    issn = {0278-2626},
    doi = {https://doi.org/10.1016/j.bandc.2019.05.007},
    url = {https://www.sciencedirect.com/science/article/pii/S0278262618304299},
    author = {Francesca Delogu and Harm Brouwer and Matthew W. Crocker},
    keywords = {Discourse comprehension, Event-Related Potentials (ERPs), N400, P600, Semantic integration},
    abstract = {The functional interpretation of two salient language-sensitive ERP components – the N400 and the P600 – remains a matter of debate. Prominent alternative accounts link the N400 to processes related to lexical retrieval, semantic integration, or both, while the P600 has been associated with syntactic reanalysis or, alternatively, to semantic integration. The often overlapping predictions of these competing accounts in extant experimental designs, however, has meant that previous findings have failed to clearly decide among them. Here, we present an experiment that directly tests the competing hypotheses using a design that clearly teases apart the retrieval versus integration view of the N400, while also dissociating a syntactic reanalysis/reprocessing account of the P600 from semantic integration. Our findings provide support for an integrated functional interpretation according to which the N400 reflects context-sensitive lexical retrieval – but not integration – processes. While the observed P600 effects were not predicted by any account, we argue that they can be reconciled with the integration view, if spatio-temporal overlap of ERP components is taken into consideration.}
}

@book{brennan2022language,
    title={Language and the Brain: A Slim Guide to Neurolinguistics},
    author={Brennan, J.R.},
    isbn={9780198814757},
    lccn={2021947523},
    series={Oxford linguistics},
    url={https://books.google.de/books?id=KLe5zgEACAAJ},
    year={2022},
    publisher={Oxford University Press}
}


@incollection{ERPneuro,
    title = {Chapter 5 - Event-Related Potential (ERP) Research in Neurolinguistics: Part I: Techniques and Applications to Lexical Access},
    editor = {Brigitte Stemmer and Harry A. Whitaker},
    booktitle = {Handbook of Neurolinguistics},
    publisher = {Academic Press},
    address = {San Diego},
    pages = {95-109},
    year = {1998},
    isbn = {978-0-12-666055-5},
    doi = {https://doi.org/10.1016/B978-012666055-5/50009-5},
    url = {https://www.sciencedirect.com/science/article/pii/B9780126660555500095},
    author = {Sidney J. Segalowitz and Hélène Chevalier},
    abstract = {The past decade or so has seen a very large increase in the use of event-related potential (ERP) techniques to study the brain’s responses to the processing of linguistic information. In this chapter, we discuss some of the strengths and weaknesses of ERP techniques in neurolinguistic research and the ways they have been applied to brain theory. We also review specific ERP paradigms addressing questions of lexical access, word-class distinctions (open versus closed class and noun versus verb), and phonological access.}
}

@article{handsfreesegmentation,
    doi = {10.48550/ARXIV.2204.10815},

    url = {https://arxiv.org/abs/2204.10815},

    author = {Islam, Md Mofijul and Aguilar, Gustavo and Ponnusamy, Pragaash and Mathialagan, Clint Solomon and Ma, Chengyuan and Guo, Chenlei},

    keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},

    title = {A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning},

    publisher = {arXiv},

    year = {2022},

    copyright = {arXiv.org perpetual, non-exclusive license}
}


@book{inputoutput,
    title =     {How Languages Are Learned},
    author =    {Patsy M. Lightbown, Nina Spada},
    publisher = {Oxford University Press},
    isbn =      {9780194541268},
    year =      {2013},
    edition =   {4th},
    pages = {114},
    url =    {https://global.oup.com/academic/product/how-languages-are-learned-9780194406291?lang=en&cc=gb}
}

@book{brownlee2020imbalanced,
    title={Imbalanced Classification with Python: Better Metrics, Balance Skewed Classes, Cost-Sensitive Learning},
    author={Brownlee, J.},
    url={https://books.google.de/books?id=jaXJDwAAQBAJ},
    year={2020},
    publisher={Machine Learning Mastery}
}


@article{humanmachinelearning2,
    author    = {Niklas K{\"{u}}hl and
               Marc Goutier and
               Lucas Baier and
               Clemens Wolff and
               Dominik Martin},
    title     = {Human vs. supervised machine learning: Who learns patterns faster?},
    journal   = {CoRR},
    volume    = {abs/2012.03661},
    year      = {2020},
    url       = {https://arxiv.org/abs/2012.03661},
    eprinttype = {arXiv},
    eprint    = {2012.03661},
    timestamp = {Wed, 09 Dec 2020 15:29:05 +0100},
    biburl    = {https://dblp.org/rec/journals/corr/abs-2012-03661.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{humanmachinelearning1,
title =     {Human and Machine Learning: Visible, Explainable, Trustworthy and Transparent},
author =    {Jianlong Zhou, Fang Chen},
publisher = {Springer},
isbn =      {3319904027},
year =      {2018},
series =    {Human–Computer Interaction Series},
edition =   {1st ed. 2018},
}

@inproceedings{itaverbs,
    title = {Verb-particle Constructions and Prefixed Verbs in Italian:
Typology,Diachrony and Semantics},
    author = {Iacobini, Claudio and Masini, Francesca},
    booktitle = {Proceedings of the Fifth Mediterranean Morphology Meeting (MMM5)},
    month = {9},
    year = {2005},
    url = {https://www.academia.edu/1183339/Verb_particle_constructions_and_prefixed_verbs_in_Italian_typology_diachrony_and_semantics},
    publisher = {Università degli Studi di Bologna},
}

@inproceedings{TREEBANKFROMDB,
    title = {Building a Morphological Treebank for {G}erman from a Linguistic Database},
    author = {Steiner, Petra  and Ruppenhofer, Josef},
    booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
    month = {5},
    year = {2018},
    address = {Miyazaki, Japan},
    publisher = {European Language Resources Association (ELRA)},
    url = {https://aclanthology.org/L18-1613}
}

@misc{finverbs,
    author       = {Urpo Nikanne},
    title        = {{Finite sentences in Finnish: Word order,
    morphology, and information structure}},
    month        = {12},
    year         = {2017},
    publisher    = {Language Science Press},
    doi          = {10.5281/zenodo.1117710},
    url          = {https://doi.org/10.5281/zenodo.1117710}
}

@inproceedings{FLOTA,
    title =     {An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers},
    author = {Hofmann, Valentin  and
      Schuetze, Hinrich  and
      Pierrehumbert, Janet},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
    month = {5},
    year = {2022},
    address = {Dublin, Ireland},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2022.acl-short.43},
    doi = {10.18653/v1/2022.acl-short.43},
    pages = {385--393},
    abstract = {We introduce FLOTA (Few Longest Token Approximation), a simple yet effective method to improve the tokenization of pretrained language models (PLMs). FLOTA uses the vocabulary of a standard tokenizer but tries to preserve the morphological structure of words during tokenization. We evaluate FLOTA on morphological gold segmentations as well as a text classification task, using BERT, GPT-2, and XLNet as example PLMs. FLOTA leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise.},
}

@InProceedings{GERPARCOR,
    author         = {Abrami, Giuseppe and Bagci, Mevl\"{u}t and Hammerla, Leon and Mehler, Alexander},
    title          = {German Parliamentary Corpus (GerParCor)},
    booktitle      = {Proceedings of the Language Resources and Evaluation Conference},
    month          = {6},
    year           = {2022},
    address        = {Marseille, France},
    publisher      = {European Language Resources Association},
    pages          = {1900--1906},
    url            = {https://aclanthology.org/2022.lrec-1.202}
}

@ARTICLE{SCIPY,
    author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
    title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
    journal = {Nature Methods},
    year    = {2020},
    volume  = {17},
    pages   = {261--272},
    adsurl  = {https://rdcu.be/b08Wh},
    doi     = {10.1038/s41592-019-0686-2}
}

@inproceedings{hofmann-etal-2020-dagobert,
    title = {{D}ago{BERT}: {G}enerating Derivational Morphology with a Pretrained Language Model},
    author = {Hofmann, Valentin  and Pierrehumbert, Janet  and Sch{\"u}tze, Hinrich},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    month = {11},
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.emnlp-main.316},
    doi = {10.18653/v1/2020.emnlp-main.316},
    pages = {3848--3861},
    abstract = {Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT{'}s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG). Furthermore, our experiments show that the input segmentation crucially impacts BERT{'}s derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.}
}

@article{POSPLUSML,
    title = {Linguistically-augmented perplexity-based data selection for language models},
    journal = {Computer Speech & Language},
    volume = {32},
    number = {1},
    pages = {11-26},
    year = {2015},
    note = {Hybrid Machine Translation: integration of linguistics and statistics},
    issn = {0885-2308},
    doi = {https://doi.org/10.1016/j.csl.2014.10.002},
    url = {https://www.sciencedirect.com/science/article/pii/S0885230814001016},
    author = {Antonio Toral and Pavel Pecina and Longyue Wang and Josef {van Genabith}},
    keywords = {Data selection, Language modelling, Computational linguistics},
    abstract = {This paper explores the use of linguistic information for the selection of data to train language models. We depart from the state-of-the-art method in perplexity-based data selection and extend it in order to use word-level linguistic units (i.e. lemmas, named entity categories and part-of-speech tags) instead of surface forms. We then present two methods that combine the different types of linguistic knowledge as well as the surface forms (1, naïve selection of the top ranked sentences selected by each method; 2, linear interpolation of the datasets selected by the different methods). The paper presents detailed results and analysis for four languages with different levels of morphologic complexity (English, Spanish, Czech and Chinese). The interpolation-based combination outperforms the purely statistical baseline in all the scenarios, resulting in language models with lower perplexity. In relative terms the improvements are similar regardless of the language, with perplexity reductions achieved in the range 7.72–13.02%. In absolute terms the reduction is higher for languages with high type-token ratio (Chinese, 202.16) or rich morphology (Czech, 81.53) and lower for the remaining languages, Spanish (55.2) and English (34.43 on the English side of the same parallel dataset as for Czech and 61.90 on the same parallel dataset as for Spanish).}
}

@PHDTHESIS{subwordbasedLMmorphologicallyrich,
    author       = {El-Desoky Mousa, Amr},
    othercontributors = {Ney, Hermann},
    title        = {{S}ub-word based language modeling of morphologically rich
                      languages for {LVCSR}},
    address      = {Aachen},
    publisher    = {Publikationsserver der RWTH Aachen University},
    reportid     = {RWTH-CONV-145072},
    pages        = {XI, 132 S. : graph. Darst.},
    year         = {2014},
    keywords     = {Spracherkennung (SWD)},
    cin          = {120000 / 122010},
    ddc          = {004},
    cid          = {$I:(DE-82)120000_20140620$ / $I:(DE-82)122010_20140620$},
    typ          = {PUB:(DE-HGF)11},
    urn          = {urn:nbn:de:hbz:82-opus-50850},
    url          = {https://publications.rwth-aachen.de/record/444658},
}

@inproceedings{subwordvsmorfessor,
    title = {Beyond Characters: Subword-level Morpheme Segmentation},
    author = {Peters, Ben  and
      Martins, Andre F. T.},
    booktitle = {Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology},
    month = jul,
    year = {2022},
    address = {Seattle, Washington},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2022.sigmorphon-1.14},
    doi = {10.18653/v1/2022.sigmorphon-1.14},
    pages = {131--138},
    abstract = {This paper presents DeepSPIN{'}s submissions to the SIGMORPHON 2022 Shared Task on Morpheme Segmentation. We make three submissions, all to the word-level subtask. First, we show that entmax-based sparse sequence-tosequence models deliver large improvements over conventional softmax-based models, echoing results from other tasks. Then, we challenge the assumption that models for morphological tasks should be trained at the character level by building a transformer that generates morphemes as sequences of unigram language model-induced subwords. This subword transformer outperforms all of our character-level models and wins the word-level subtask. Although we do not submit an official submission to the sentence-level subtask, we show that this subword-based approach is highly effective there as well.},
}
@article{gpt,
    author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
    url = {https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf},
    title = {Improving language understanding by generative pre-training},
    year = {2018}
}

@article{olmpics,
    author = {Talmor, Alon and Elazar, Yanai and Goldberg, Yoav and Berant, Jonathan},
    title = {oLMpics-On What Language Model Pre-training Captures},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {743-758},
    year = {2020},
    month = {12},
    abstract = {Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models, and objective functions for pre-training.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00342},
    url = {https://doi.org/10.1162/tacl\_a\_00342},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00342/1923716/tacl\_a\_00342.pdf},
}


@misc{oscar,
    author = {OSCAR},
    title = {The OSCAR project (Open Super-large Crawled Aggregated coRpus)},
    year = {2022},
    month = {1},
    url = {https://oscar-project.org/},
    comment = {Hosted on {https://huggingface.co/datasets/oscar}, last visited 08.02.2023},
    lastchecked = {09-02-2023}
}

@MISC{lehmann,
author = {Lehman, Christian W.},
title = {Indogermanisch},
month = {08},
year = {2022},
url={https://www.christianlehmann.eu/ling/sprachen/indogermania/RomGesch/idg.php},
lastchecked = {09-02-2022}
}

@inproceedings{oscarsub1,
    title = {A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages},
    author = {Ortiz Su{'a}rez, Pedro Javier  and
      Romary, Laurent  and
      Sagot, Benoit},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    month = {6},
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/2020.acl-main.156},
    pages = {1703--1714}
}

@inproceedings{oscarsub2,
    author    = {Pedro Javier {Ortiz Su{'a}rez} and Benoit Sagot and Laurent Romary},
    title     = {Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures},
    series = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019},
    editor    = {Piotr Bański and Adrien Barbaresi and Hanno Biber and Evelyn Breiteneder and Simon Clematide and Marc Kupietz and Harald L{"u}ngen and Caroline Iliadi},
    publisher = {Leibniz-Institut f{"u}r Deutsche Sprache},
    address   = {Mannheim},
    doi       = {10.14618/ids-pub-9021},
    url       = {http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215},
    pages     = {9 -- 16},
    year      = {2019},
    language  = {en}
}

@article{morfessor,
    author    = {Mathias Creutz and
               Krista Lagus},
    title     = {Unsupervised Discovery of Morphemes},
    journal   = {CoRR},
    volume    = {cs.CL/0205057},
    year      = {2002},
    url       = {https://arxiv.org/abs/cs/0205057},
    timestamp = {Fri, 10 Jan 2020 12:58:28 +0100},
    biburl    = {https://dblp.org/rec/journals/corr/cs-CL-0205057.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{wiktionary,
    title={Verzeichnis: Deutsch},
    url={https://de.wiktionary.org/wiki/Verzeichnis:Deutsch},
    journal={Wiktionary - das freie Wörterbuch},
    publisher={Creative Commons}, author={Wiktionary},
    year={2022},
    month={9}}

@inproceedings{hanta,
    author    = {Christian Wartena},
    title     = {A Probabilistic Morphology Model for German Lemmatization},
    series = {Proceedings of the 15th Conference on Natural Language Processing (KONVENS 2019)},
    doi       = {10.25968/opus-1527},
    url       = {http://nbn-resolving.de/urn:nbn:de:bsz:960-opus4-15271},
    pages     = {40 -- 49},
    year      = {2019},
    abstract  = {Lemmatization is a central task in many NLP applications. Despite this importance, the number of (freely) available and easy to use tools for German is very limited. To fill this gap, we developed a simple lemmatizer that can be trained on any lemmatized corpus. For a full form word the tagger tries to find the sequence of morphemes that is most likely to generate that word. From this sequence of tags we can easily derive the stem, the lemma and the part of speech (PoS) of the word. We show (i) that the quality of this approach is comparable to state of the art methods and (ii) that we can improve the results of Part-of-Speech (PoS) tagging when we include the morphological analysis of each word.},
    language  = {en}
}