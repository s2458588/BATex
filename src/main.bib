%! Author = gnom
%! Date = 08.12.22

@book{METZLER2016,
    editor={Gl{\"u}ck, Helmut and R{\"o}del, Michael},
    title={Metzler Lexikon Sprache},
    edition={5},
    publisher={J.B. Metzler},
    address={Stuttgart},
    year={2016},
    pages={141-142},
    language={ger},
    isbn={978-3-476-05486-9},
    series={Springer eBook Collection},
    doi={10.1007/978-3-476-05486-9},
    keywords={Linguistik},
    url={http://dx.doi.org/10.1007/978-3-476-05486-9},
    library={UB},
}


% TOKENIZATION INFLUENCES MODEL: VOCAB SIZE 20-40% PARAMETER SIZE
@misc{TOKENIZATIONIMPACT,
    doi = {10.48550/ARXIV.2204.08832},
    url = {https://arxiv.org/abs/2204.08832},
    author = {Toraman, Cagri and Yilmaz, Eyup Halit and Şahinuç, Furkan and Ozcelik, Oguzhan},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Impact of Tokenization on Language Models: An Analysis for Turkish},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

% INTRODUCTION OF TRANSFORMER ARCHITECTURE
@misc{ATTENTION,
    doi = {10.48550/ARXIV.1706.03762},
    url = {https://arxiv.org/abs/1706.03762},
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
    keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Attention Is All You Need},
    publisher = {arXiv},
    year = {2017},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

% BERT HIGH BENCHMARK PERFORMANCE
@misc{BERTHIGH1,
    doi = {10.48550/ARXIV.1810.04805},
    url = {https://arxiv.org/abs/1810.04805},
    author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}


% DEEPL STATEMENT
@online{DEEPL,
    title={How does deepl work?},
    url={https://www.deepl.com/en/blog/how-does-deepl-work},
    journal={DeepL Translate: The world's most accurate translator},
    publisher={DeepL SE},
    author={DeepL},
    year={2021},
    month={11},
    addendum = {Last accessed: 28.12.2022},
}

% ORIGINAL IDEA FOR WORDPIECE TOKENIZATION ALGORITHM
@INPROCEEDINGS{WORDPIECEOG,
    author={Schuster, Mike and Nakajima, Kaisuke},
    booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
    title={Japanese and Korean voice search},
    year={2012},
    volume={},
    number={},
    pages={5149-5152},
    doi={10.1109/ICASSP.2012.6289079}}

% GOOGLE INTRODUING Neural Machine Translation (NMT) and WORDPIECE (FROM WORDPIECEOG)
@misc{WORDPIECEGOOGLE,
    doi = {10.48550/ARXIV.1609.08144},
    url = {https://arxiv.org/abs/1609.08144},
    author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Łukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
    keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
    publisher = {arXiv},
    year = {2016},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

% BYTE PAIR ENCODING ORIGINAL PAPER
@inproceedings{BPE,
    title = {Neural Machine Translation of Rare Words with Subword Units},
    author = {Sennrich, Rico  and Haddow, Barry  and Birch, Alexandra},
    booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = {8},
    year = {2016},
    address = {Berlin, Germany},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/P16-1162},
    doi = {10.18653/v1/P16-1162},
    pages = {1715--1725},
}

% MORPHOLOGY: POLYSYNTHETIC LANGUAGE MODELING (CONTAINS MORPHESSOR)
@misc{POLYSYNTHLM,
    doi = {10.48550/ARXIV.2005.05477},
    url = {https://arxiv.org/abs/2005.05477},
    author = {Schwartz, Lane and Tyers, Francis and Levin, Lori and Kirov, Christo and Littell, Patrick and Lo, Chi-kiu and Prud'hommeaux, Emily and Park, Hyunji Hayley and Steimel, Kenneth and Knowles, Rebecca and Micher, Jeffrey and Strunk, Lonny and Liu, Han and Haley, Coleman and Zhang, Katherine J. and Jimmerson, Robbie and Andriyanets, Vasilisa and Muis, Aldrian Obaja and Otani, Naoki and Park, Jong Hyuk and Zhang, Zhisong},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Neural Polysynthetic Language Modelling},
    publisher = {arXiv},
    year = {2020},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

% Base-Inflection Encoding: FUNCTIONAL MORPHEMES BECOME TOKENS
@inproceedings{BITE,
    title = {Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding},
    author = {Tan, Samson  and
      Joty, Shafiq  and
      Varshney, Lav  and
      Kan, Min-Yen},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    month = {11},
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.emnlp-main.455},
    doi = {10.18653/v1/2020.emnlp-main.455},
    pages = {5647--5663},
    abstract = {Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English. Although comprehension by human readers is usually unimpaired by non-standard inflections, current NLP systems are not yet robust. We propose Base-Inflection Encoding (BITE), a method to tokenize English text by reducing inflected words to their base forms before reinjecting the grammatical information as special symbols. Fine-tuning pretrained NLP models for downstream tasks using our encoding defends against inflectional adversaries while maintaining performance on clean data. Models using BITE generalize better to dialects with non-standard inflections without explicit training and translation models converge faster when trained with BITE. Finally, we show that our encoding improves the vocabulary efficiency of popular data-driven subword tokenizers. Since there has been no prior work on quantitatively evaluating vocabulary efficiency, we propose metrics to do so.},
}

@misc{MORPHEMEDISCOVERYUNSUPERVISED,
    doi = {10.48550/ARXIV.CS/0205057},
    url = {https://arxiv.org/abs/cs/0205057},
    author = {Creutz, Mathias and Lagus, Krista},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7},
    title = {Unsupervised Discovery of Morphemes},
    publisher = {arXiv},
    year = {2002},
    copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004}
}



@article{IOINTERACTION,
    author ={Gass, Susan M. and Mackey, Alison},
    title ={Input, Interaction and Output: An Overview},
    journal={AILA Review},
    year ={2006},
    volume ={19},
    number ={1},
    pages ={3-17},
    doi ={https://doi.org/10.1075/aila.19.03gas},
    url ={https://www.jbe-platform.com/content/journals/10.1075/aila.19.03gas},
    publisher ={John Benjamins},
    issn ={1461-0213},
    type ={Journal Article},
    abstract ={This paper presents an overview of what has come to be known as the Interaction Hypothesis, the basic tenet of which is that through input and interaction with interlocutors, language learners have opportunities to notice differences between their own formulations of the target language and the language of their conversational partners. They also receive feedback which both modifies the linguistic input they receive and pushes them to modify their output during conversation. This paper focuses on the major constructs of this approach to SLA, namely, input, interaction, feedback and output, and discusses recent literature that addresses these issues.},
}

@article{BPEGAGE,
    author = {Gage, Philip},
    title = {A New Algorithm for Data Compression},
    year = {1994},
    issue_date = {Feb. 1994},
    publisher = {R & D Publications, Inc.},
    address = {USA},
    volume = {12},
    number = {2},
    issn = {0898-9788},
    journal = {C Users J.},
    month = {2},
    pages = {23–38},
    numpages = {16}
}

@article{LSTM,
    author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
    title = {Long Short-Term Memory},
    year = {1997},
    issue_date = {November 15, 1997},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    volume = {9},
    number = {8},
    issn = {0899-7667},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    doi = {10.1162/neco.1997.9.8.1735},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
    journal = {Neural Comput.},
    month = {11},
    pages = {1735–1780},
    numpages = {46}
}

@book{WALSGEN,
    address   = {Leipzig},
    editor    = {Matthew S. Dryer and Martin Haspelmath},
    publisher = {Max Planck Institute for Evolutionary Anthropology},
    title     = {WALS Online},
    url       = {https://wals.info/},
    year      = {2013}
}

@incollection{WALSFUS,
    address   = {Leipzig},
    author    = {Balthasar Bickel and Johanna Nichols},
    booktitle = {The World Atlas of Language Structures Online},
    editor    = {Matthew S. Dryer and Martin Haspelmath},
    publisher = {Max Planck Institute for Evolutionary Anthropology},
    title     = {Fusion of Selected Inflectional Formatives},
    url       = {https://wals.info/chapter/20},
    year      = {2013}
}

% DEFINITION OF MORPHEME
@misc{morpheme,
    author = {Andrew M. Colman},
    title = {morpheme},
    year = {2009},
    publisher = {Oxford University Press},
    isbn = {9780191726828},
    doi = {10.1093/acref/9780199534067.013.5219},
    url = {https://www.oxfordreference.com/view/10.1093/acref/9780199534067.001.0001/acref-9780199534067-e-5219}
}

@book{MORPHOLOGICALCOMPLEXITY,
    editor = {Baerman, Matthew and Brown, Dunstan and Corbett, Greville G.},
    title = {Morphological Complexity},
    %edition = {1},
    publisher = {Cambridge University Press},
    adress = {Cambridge},
    year = {2017},
    %language = {english},
    isbn = {1107120640},
    series = {Cambridge studies in Linguistics 153},
    doi = {10.1017/9781316343074},
    %keywords = {Morphology},
    url = {www.cambridge.org/9781107120648},
    library = {British Library}
}

% MORPHOLOGY
@inbook{FROMWORDTOPARADIGM,
    place={Cambridge},
    series={Cambridge Studies in Linguistics},
    booktitle={Morphological Typology: From Word to Paradigm},
    publisher={Cambridge University Press},
    author={Stump, Gregory and Finkel, Raphael A.},
    year={2013}, pages={iii-iv},
    collection={Cambridge Studies in Linguistics}
}

% MORPHOLOGY
@book{MMM4,
    editor = {Booij, Geert and Guevara, Emiliano and Ralli, Angeliki and Sgroi, Salvatore and Scalise, Sergio},
    year = {2003},
    month = {09},
    pages = {},
    title = {Morphology and linguistic typology. On-line Proceedings of the Fourth Mediterranean Morphology Meeting MMM4}
}

% MORPHOLOGY
@book{LINGUISTICTYPOLOGY,
    author = {Aikhenvald, Alexandra and Dixon, R},
    address={Cambridge},
    series={Cambridge Handbooks in Language and Linguistics},
    title={The Cambridge Handbook of Linguistic Typology},
    DOI={10.1017/9781316135716},
    publisher={Cambridge University Press},
    year={2017}, collection={Cambridge Handbooks in Language and Linguistics}
}

% WORD MORPHOLOGY
@article{WORDTYPOLOGY,
    author = {Dixon, R. and Aikhenvald, Alexandra},
    year = {2002},
    month = {01},
    pages = {},
    title = {Word: A cross - linguistic typology},
    isbn = {9780521818995},
    doi = {10.1017/CBO9780511486241}
}

@inproceedings{TREEBANKFROMDB,
    title = {Building a Morphological Treebank for {G}erman from a Linguistic Database},
    author = {Steiner, Petra  and Ruppenhofer, Josef},
    booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
    month = {5},
    year = {2018},
    address = {Miyazaki, Japan},
    publisher = {European Language Resources Association (ELRA)},
    url = {https://aclanthology.org/L18-1613}
}

@inproceedings{FLOTA,
    title =     {An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers},
    author = {Hofmann, Valentin  and
      Schuetze, Hinrich  and
      Pierrehumbert, Janet},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
    month = {5},
    year = {2022},
    address = {Dublin, Ireland},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2022.acl-short.43},
    doi = {10.18653/v1/2022.acl-short.43},
    pages = {385--393},
    abstract = {We introduce FLOTA (Few Longest Token Approximation), a simple yet effective method to improve the tokenization of pretrained language models (PLMs). FLOTA uses the vocabulary of a standard tokenizer but tries to preserve the morphological structure of words during tokenization. We evaluate FLOTA on morphological gold segmentations as well as a text classification task, using BERT, GPT-2, and XLNet as example PLMs. FLOTA leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise.},
}

@InProceedings{GERPARCOR,
    author         = {Abrami, Giuseppe and Bagci, Mevl\"{u}t and Hammerla, Leon and Mehler, Alexander},
    title          = {German Parliamentary Corpus (GerParCor)},
    booktitle      = {Proceedings of the Language Resources and Evaluation Conference},
    month          = {6},
    year           = {2022},
    address        = {Marseille, France},
    publisher      = {European Language Resources Association},
    pages          = {1900--1906},
    url            = {https://aclanthology.org/2022.lrec-1.202}
}

@ARTICLE{SCIPY,
    author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
    title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
    journal = {Nature Methods},
    year    = {2020},
    volume  = {17},
    pages   = {261--272},
    adsurl  = {https://rdcu.be/b08Wh},
    doi     = {10.1038/s41592-019-0686-2}
}

@inproceedings{hofmann-etal-2020-dagobert,
    title = {{D}ago{BERT}: {G}enerating Derivational Morphology with a Pretrained Language Model},
    author = {Hofmann, Valentin  and Pierrehumbert, Janet  and Sch{\"u}tze, Hinrich},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    month = {11},
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.emnlp-main.316},
    doi = {10.18653/v1/2020.emnlp-main.316},
    pages = {3848--3861},
    abstract = {Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT{'}s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG). Furthermore, our experiments show that the input segmentation crucially impacts BERT{'}s derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.}
}