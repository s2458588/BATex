%! Author = gnom
%! Date = 08.12.22

@book{METZLER2016,
    editor={Gl{\"u}ck, Helmut and R{\"o}del, Michael},
    title={Metzler Lexikon Sprache},
    edition={5},
    publisher={J.B. Metzler},
    address={Stuttgart},
    year={2016},
    pages={141-142},
    language={ger},
    isbn={978-3-476-05486-9},
    series={Springer eBook Collection},
    doi={10.1007/978-3-476-05486-9},
    keywords={Linguistik},
    url={http://dx.doi.org/10.1007/978-3-476-05486-9},
    library={UB},
}


% TOKENIZATION INFLUENCES MODEL: VOCAB SIZE 20-40% PARAMETER SIZE
@misc{TOKENIZATIONIMPACT,
    doi = {10.48550/ARXIV.2204.08832},
    url = {https://arxiv.org/abs/2204.08832},
    author = {Toraman, Cagri and Yilmaz, Eyup Halit and Şahinuç, Furkan and Ozcelik, Oguzhan},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Impact of Tokenization on Language Models: An Analysis for Turkish},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

% INTRODUCTION OF TRANSFORMER ARCHITECTURE
@misc{ATTENTION,
    doi = {10.48550/ARXIV.1706.03762},
    url = {https://arxiv.org/abs/1706.03762},
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
    keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Attention Is All You Need},
    publisher = {arXiv},
    year = {2017},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

% BERT HIGH BENCHMARK PERFORMANCE
@misc{BERTHIGH1,
    doi = {10.48550/ARXIV.1810.04805},
    url = {https://arxiv.org/abs/1810.04805},
    author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}


% DEEPL STATEMENT
@online{DEEPL,
    title={How does deepl work?},
    url={https://www.deepl.com/en/blog/how-does-deepl-work},
    journal={DeepL Translate: The world's most accurate translator},
    publisher={DeepL SE},
    author={DeepL},
    year={2021},
    month={11},
    addendum = {Last accessed: 28.12.2022},
}

% ORIGINAL IDEA FOR WORDPIECE TOKENIZATION ALGORITHM
@INPROCEEDINGS{WORDPIECEOG,
    author={Schuster, Mike and Nakajima, Kaisuke},
    booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
    title={Japanese and Korean voice search},
    year={2012},
    volume={},
    number={},
    pages={5149-5152},
    doi={10.1109/ICASSP.2012.6289079}}

% GOOGLE INTRODUING Neural Machine Translation (NMT) and WORDPIECE (FROM WORDPIECEOG)
@misc{WORDPIECEGOOGLE,
    doi = {10.48550/ARXIV.1609.08144},
    url = {https://arxiv.org/abs/1609.08144},
    author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Łukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
    keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
    publisher = {arXiv},
    year = {2016},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

% BYTE PAIR ENCODING ORIGINAL PAPER
@inproceedings{BPE,
    title = {Neural Machine Translation of Rare Words with Subword Units},
    author = {Sennrich, Rico  and Haddow, Barry  and Birch, Alexandra},
    booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = {8},
    year = {2016},
    address = {Berlin, Germany},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/P16-1162},
    doi = {10.18653/v1/P16-1162},
    pages = {1715--1725},
}

% MORPHOLOGY: POLYSYNTHETIC LANGUAGE MODELING (CONTAINS MORPHESSOR)
@misc{POLYSYNTHLM,
    doi = {10.48550/ARXIV.2005.05477},
    url = {https://arxiv.org/abs/2005.05477},
    author = {Schwartz, Lane and Tyers, Francis and Levin, Lori and Kirov, Christo and Littell, Patrick and Lo, Chi-kiu and Prud'hommeaux, Emily and Park, Hyunji Hayley and Steimel, Kenneth and Knowles, Rebecca and Micher, Jeffrey and Strunk, Lonny and Liu, Han and Haley, Coleman and Zhang, Katherine J. and Jimmerson, Robbie and Andriyanets, Vasilisa and Muis, Aldrian Obaja and Otani, Naoki and Park, Jong Hyuk and Zhang, Zhisong},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Neural Polysynthetic Language Modelling},
    publisher = {arXiv},
    year = {2020},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

% Base-Inflection Encoding: FUNCTIONAL MORPHEMES BECOME TOKENS
@inproceedings{BITE,
    title = {Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding},
    author = {Tan, Samson  and
      Joty, Shafiq  and
      Varshney, Lav  and
      Kan, Min-Yen},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    month = {11},
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.emnlp-main.455},
    doi = {10.18653/v1/2020.emnlp-main.455},
    pages = {5647--5663},
    abstract = {Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English. Although comprehension by human readers is usually unimpaired by non-standard inflections, current NLP systems are not yet robust. We propose Base-Inflection Encoding (BITE), a method to tokenize English text by reducing inflected words to their base forms before reinjecting the grammatical information as special symbols. Fine-tuning pretrained NLP models for downstream tasks using our encoding defends against inflectional adversaries while maintaining performance on clean data. Models using BITE generalize better to dialects with non-standard inflections without explicit training and translation models converge faster when trained with BITE. Finally, we show that our encoding improves the vocabulary efficiency of popular data-driven subword tokenizers. Since there has been no prior work on quantitatively evaluating vocabulary efficiency, we propose metrics to do so.},
}

@misc{MORPHEMEDISCOVERYUNSUPERVISED,
    doi = {10.48550/ARXIV.CS/0205057},
    url = {https://arxiv.org/abs/cs/0205057},
    author = {Creutz, Mathias and Lagus, Krista},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7},
    title = {Unsupervised Discovery of Morphemes},
    publisher = {arXiv},
    year = {2002},
    copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004}
}



@article{IOINTERACTION,
    author ={Gass, Susan M. and Mackey, Alison},
    title ={Input, Interaction and Output: An Overview},
    journal={AILA Review},
    year ={2006},
    volume ={19},
    number ={1},
    pages ={3-17},
    doi ={https://doi.org/10.1075/aila.19.03gas},
    url ={https://www.jbe-platform.com/content/journals/10.1075/aila.19.03gas},
    publisher ={John Benjamins},
    issn ={1461-0213},
    type ={Journal Article},
    abstract ={This paper presents an overview of what has come to be known as the Interaction Hypothesis, the basic tenet of which is that through input and interaction with interlocutors, language learners have opportunities to notice differences between their own formulations of the target language and the language of their conversational partners. They also receive feedback which both modifies the linguistic input they receive and pushes them to modify their output during conversation. This paper focuses on the major constructs of this approach to SLA, namely, input, interaction, feedback and output, and discusses recent literature that addresses these issues.},
}

@article{BPEGAGE,
    author = {Gage, Philip},
    title = {A New Algorithm for Data Compression},
    year = {1994},
    issue_date = {Feb. 1994},
    publisher = {R & D Publications, Inc.},
    address = {USA},
    volume = {12},
    number = {2},
    issn = {0898-9788},
    journal = {C Users J.},
    month = {2},
    pages = {23–38},
    numpages = {16}
}

@article{LSTM,
    author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
    title = {Long Short-Term Memory},
    year = {1997},
    issue_date = {November 15, 1997},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    volume = {9},
    number = {8},
    issn = {0899-7667},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    doi = {10.1162/neco.1997.9.8.1735},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
    journal = {Neural Comput.},
    month = {11},
    pages = {1735–1780},
    numpages = {46}
}

@book{WALSGEN,
    address   = {Leipzig},
    editor    = {Matthew S. Dryer and Martin Haspelmath},
    publisher = {Max Planck Institute for Evolutionary Anthropology},
    title     = {WALS Online},
    url       = {https://wals.info/},
    year      = {2013}
}

@incollection{WALSFUS,
    address   = {Leipzig},
    author    = {Balthasar Bickel and Johanna Nichols},
    booktitle = {The World Atlas of Language Structures Online},
    editor    = {Matthew S. Dryer and Martin Haspelmath},
    publisher = {Max Planck Institute for Evolutionary Anthropology},
    title     = {Fusion of Selected Inflectional Formatives},
    url       = {https://wals.info/chapter/20},
    year      = {2013}
}

% DEFINITION OF MORPHEME
@misc{morpheme,
    author = {Andrew M. Colman},
    title = {morpheme},
    year = {2009},
    publisher = {Oxford University Press},
    isbn = {9780191726828},
    doi = {10.1093/acref/9780199534067.013.5219},
    url = {https://www.oxfordreference.com/view/10.1093/acref/9780199534067.001.0001/acref-9780199534067-e-5219}
}

@book{MORPHOLOGICALCOMPLEXITY,
    editor = {Baerman, Matthew and Brown, Dunstan and Corbett, Greville G.},
    title = {Morphological Complexity},
    %edition = {1},
    publisher = {Cambridge University Press},
    adress = {Cambridge},
    year = {2017},
    %language = {english},
    isbn = {1107120640},
    series = {Cambridge studies in Linguistics 153},
    doi = {10.1017/9781316343074},
    %keywords = {Morphology},
    url = {www.cambridge.org/9781107120648},
    library = {British Library}
}

% MORPHOLOGY
@inbook{FROMWORDTOPARADIGM,
    place={Cambridge},
    series={Cambridge Studies in Linguistics},
    booktitle={Morphological Typology: From Word to Paradigm},
    publisher={Cambridge University Press},
    author={Stump, Gregory and Finkel, Raphael A.},
    year={2013}, pages={iii-iv},
    collection={Cambridge Studies in Linguistics}
}

% MORPHOLOGY
@book{MMM4,
    editor = {Booij, Geert and Guevara, Emiliano and Ralli, Angeliki and Sgroi, Salvatore and Scalise, Sergio},
    year = {2003},
    month = {09},
    pages = {},
    title = {Morphology and linguistic typology. On-line Proceedings of the Fourth Mediterranean Morphology Meeting MMM4}
}

% MORPHOLOGY
@book{LINGUISTICTYPOLOGY,
    author = {Aikhenvald, Alexandra and Dixon, R},
    address={Cambridge},
    series={Cambridge Handbooks in Language and Linguistics},
    title={The Cambridge Handbook of Linguistic Typology},
    DOI={10.1017/9781316135716},
    publisher={Cambridge University Press},
    year={2017}, collection={Cambridge Handbooks in Language and Linguistics}
}

% WORD MORPHOLOGY
@article{WORDTYPOLOGY,
    author = {Dixon, R. and Aikhenvald, Alexandra},
    year = {2002},
    month = {01},
    pages = {},
    title = {Word: A cross - linguistic typology},
    isbn = {9780521818995},
    doi = {10.1017/CBO9780511486241}
}

@inproceedings{TREEBANKFROMDB,
    title = {Building a Morphological Treebank for {G}erman from a Linguistic Database},
    author = {Steiner, Petra  and Ruppenhofer, Josef},
    booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
    month = {5},
    year = {2018},
    address = {Miyazaki, Japan},
    publisher = {European Language Resources Association (ELRA)},
    url = {https://aclanthology.org/L18-1613}
}

@inproceedings{FLOTA,
    title =     {An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers},
    author = {Hofmann, Valentin  and
      Schuetze, Hinrich  and
      Pierrehumbert, Janet},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
    month = {5},
    year = {2022},
    address = {Dublin, Ireland},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2022.acl-short.43},
    doi = {10.18653/v1/2022.acl-short.43},
    pages = {385--393},
    abstract = {We introduce FLOTA (Few Longest Token Approximation), a simple yet effective method to improve the tokenization of pretrained language models (PLMs). FLOTA uses the vocabulary of a standard tokenizer but tries to preserve the morphological structure of words during tokenization. We evaluate FLOTA on morphological gold segmentations as well as a text classification task, using BERT, GPT-2, and XLNet as example PLMs. FLOTA leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise.},
}

@InProceedings{GERPARCOR,
    author         = {Abrami, Giuseppe and Bagci, Mevl\"{u}t and Hammerla, Leon and Mehler, Alexander},
    title          = {German Parliamentary Corpus (GerParCor)},
    booktitle      = {Proceedings of the Language Resources and Evaluation Conference},
    month          = {6},
    year           = {2022},
    address        = {Marseille, France},
    publisher      = {European Language Resources Association},
    pages          = {1900--1906},
    url            = {https://aclanthology.org/2022.lrec-1.202}
}

@ARTICLE{SCIPY,
    author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
    title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
    journal = {Nature Methods},
    year    = {2020},
    volume  = {17},
    pages   = {261--272},
    adsurl  = {https://rdcu.be/b08Wh},
    doi     = {10.1038/s41592-019-0686-2}
}

@inproceedings{hofmann-etal-2020-dagobert,
    title = {{D}ago{BERT}: {G}enerating Derivational Morphology with a Pretrained Language Model},
    author = {Hofmann, Valentin  and Pierrehumbert, Janet  and Sch{\"u}tze, Hinrich},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    month = {11},
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.emnlp-main.316},
    doi = {10.18653/v1/2020.emnlp-main.316},
    pages = {3848--3861},
    abstract = {Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT{'}s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG). Furthermore, our experiments show that the input segmentation crucially impacts BERT{'}s derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.}
}

@article{POSPLUSML,
    title = {Linguistically-augmented perplexity-based data selection for language models},
    journal = {Computer Speech & Language},
    volume = {32},
    number = {1},
    pages = {11-26},
    year = {2015},
    note = {Hybrid Machine Translation: integration of linguistics and statistics},
    issn = {0885-2308},
    doi = {https://doi.org/10.1016/j.csl.2014.10.002},
    url = {https://www.sciencedirect.com/science/article/pii/S0885230814001016},
    author = {Antonio Toral and Pavel Pecina and Longyue Wang and Josef {van Genabith}},
    keywords = {Data selection, Language modelling, Computational linguistics},
    abstract = {This paper explores the use of linguistic information for the selection of data to train language models. We depart from the state-of-the-art method in perplexity-based data selection and extend it in order to use word-level linguistic units (i.e. lemmas, named entity categories and part-of-speech tags) instead of surface forms. We then present two methods that combine the different types of linguistic knowledge as well as the surface forms (1, naïve selection of the top ranked sentences selected by each method; 2, linear interpolation of the datasets selected by the different methods). The paper presents detailed results and analysis for four languages with different levels of morphologic complexity (English, Spanish, Czech and Chinese). The interpolation-based combination outperforms the purely statistical baseline in all the scenarios, resulting in language models with lower perplexity. In relative terms the improvements are similar regardless of the language, with perplexity reductions achieved in the range 7.72–13.02%. In absolute terms the reduction is higher for languages with high type-token ratio (Chinese, 202.16) or rich morphology (Czech, 81.53) and lower for the remaining languages, Spanish (55.2) and English (34.43 on the English side of the same parallel dataset as for Czech and 61.90 on the same parallel dataset as for Spanish).}
}

@PHDTHESIS{subwordbasedLMmorphologicallyrich,
    author       = {El-Desoky Mousa, Amr},
    othercontributors = {Ney, Hermann},
    title        = {{S}ub-word based language modeling of morphologically rich
                      languages for {LVCSR}},
    address      = {Aachen},
    publisher    = {Publikationsserver der RWTH Aachen University},
    reportid     = {RWTH-CONV-145072},
    pages        = {XI, 132 S. : graph. Darst.},
    year         = {2014},
    keywords     = {Spracherkennung (SWD)},
    cin          = {120000 / 122010},
    ddc          = {004},
    cid          = {$I:(DE-82)120000_20140620$ / $I:(DE-82)122010_20140620$},
    typ          = {PUB:(DE-HGF)11},
    urn          = {urn:nbn:de:hbz:82-opus-50850},
    url          = {https://publications.rwth-aachen.de/record/444658},
}

@inproceedings{subwordvsmorfessor,
    title = {Beyond Characters: Subword-level Morpheme Segmentation},
    author = {Peters, Ben  and
      Martins, Andre F. T.},
    booktitle = {Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology},
    month = jul,
    year = {2022},
    address = {Seattle, Washington},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2022.sigmorphon-1.14},
    doi = {10.18653/v1/2022.sigmorphon-1.14},
    pages = {131--138},
    abstract = {This paper presents DeepSPIN{'}s submissions to the SIGMORPHON 2022 Shared Task on Morpheme Segmentation. We make three submissions, all to the word-level subtask. First, we show that entmax-based sparse sequence-tosequence models deliver large improvements over conventional softmax-based models, echoing results from other tasks. Then, we challenge the assumption that models for morphological tasks should be trained at the character level by building a transformer that generates morphemes as sequences of unigram language model-induced subwords. This subword transformer outperforms all of our character-level models and wins the word-level subtask. Although we do not submit an official submission to the sentence-level subtask, we show that this subword-based approach is highly effective there as well.},
}

@article{olmpics,
    author = {Talmor, Alon and Elazar, Yanai and Goldberg, Yoav and Berant, Jonathan},
    title = {oLMpics-On What Language Model Pre-training Captures},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {743-758},
    year = {2020},
    month = {12},
    abstract = {Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models, and objective functions for pre-training.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00342},
    url = {https://doi.org/10.1162/tacl\_a\_00342},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00342/1923716/tacl\_a\_00342.pdf},
}


@misc{oscar,
    author = {OSCAR},
    title = {The OSCAR project (Open Super-large Crawled Aggregated coRpus)},
    year = {2022},
    month = {1},
    url = {https://oscar-project.org/},
    comment = {Hosted on {https://huggingface.co/datasets/oscar}, last visited 08.02.2023}
}

@inproceedings{oscarsub1,
    title = {A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages},
    author = {Ortiz Su{'a}rez, Pedro Javier  and
      Romary, Laurent  and
      Sagot, Benoit},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    month = {6},
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/2020.acl-main.156},
    pages = {1703--1714}
}

@inproceedings{oscarsub2,
    author    = {Pedro Javier {Ortiz Su{'a}rez} and Benoit Sagot and Laurent Romary},
    title     = {Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures},
    series = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019},
    editor    = {Piotr Bański and Adrien Barbaresi and Hanno Biber and Evelyn Breiteneder and Simon Clematide and Marc Kupietz and Harald L{"u}ngen and Caroline Iliadi},
    publisher = {Leibniz-Institut f{"u}r Deutsche Sprache},
    address   = {Mannheim},
    doi       = {10.14618/ids-pub-9021},
    url       = {http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215},
    pages     = {9 -- 16},
    year      = {2019},
    language  = {en}
}

@article{morfessor,
    author    = {Mathias Creutz and
               Krista Lagus},
    title     = {Unsupervised Discovery of Morphemes},
    journal   = {CoRR},
    volume    = {cs.CL/0205057},
    year      = {2002},
    url       = {https://arxiv.org/abs/cs/0205057},
    timestamp = {Fri, 10 Jan 2020 12:58:28 +0100},
    biburl    = {https://dblp.org/rec/journals/corr/cs-CL-0205057.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{wiktionary,
    title={Verzeichnis: Deutsch},
    url={https://de.wiktionary.org/wiki/Verzeichnis:Deutsch},
    journal={Wiktionary - das freie Wörterbuch},
    publisher={Creative Commons}, author={Wiktionary},
    year={2022},
    month={9}}

@inproceedings{hanta,
    author    = {Christian Wartena},
    title     = {A Probabilistic Morphology Model for German Lemmatization},
    series = {Proceedings of the 15th Conference on Natural Language Processing (KONVENS 2019)},
    doi       = {10.25968/opus-1527},
    url       = {http://nbn-resolving.de/urn:nbn:de:bsz:960-opus4-15271},
    pages     = {40 -- 49},
    year      = {2019},
    abstract  = {Lemmatization is a central task in many NLP applications. Despite this importance, the number of (freely) available and easy to use tools for German is very limited. To fill this gap, we developed a simple lemmatizer that can be trained on any lemmatized corpus. For a full form word the tagger tries to find the sequence of morphemes that is most likely to generate that word. From this sequence of tags we can easily derive the stem, the lemma and the part of speech (PoS) of the word. We show (i) that the quality of this approach is comparable to state of the art methods and (ii) that we can improve the results of Part-of-Speech (PoS) tagging when we include the morphological analysis of each word.},
    language  = {en}
}