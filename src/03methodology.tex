in this section the whole methodoloy is covered. what do i use in this thesis, why do i use it and lastly, how?
make sure the why covers methodological implications. (vergiss nicht alle pakete als quelle im Anhang)

\section{Requirements}
\label{sec:requirements}
A series of tools will help to achieve lexicalized tokenization.
They will be explained in this chapter along with their methodological edge.

% EVTL. MACHINE LEARNING MODEL AUFSPLITTEN IN MODEL, TOKENIZER VOCABULARY?
\subsection{Learning Architecture \& Tokenizer}
\label{subsec:mlm}

\ac{bert} is a language learning transformer model designed for \ac{nlp} tasks (\cite{ATTENTION}).
Upon release it achieved higher performance scores compared to previously used \ac{lstm} models (\cite{BERTHIGH1}).
Two main model characteristics can be observed for \ac{bert}.
Firstly, it is the first \ac{lm} to implement simultaneous attention heads, allowing for bidirectional reading.
The methodological implication of reading to the left and right of a token is to include more information about the language in single embeddings.
Secondly, \ac{bert} introduced the (at the time novel) \ac{mlm} method for training.
The method involves masking a specified amount (default 15\%) of random tokens in the input sequence.
Masked tokens are guessed by the model which can then update its weights according to success or failure.

The \ac{nlp} community has since developed \ac{bert} and adapted it to the needs of contemporary \ac{nlp} problems (roberta, germanbert, mbert \uppercase{citation}).
Its wide support, comparability and versatility make \ac{bert} the model of choice for this thesis.
Another notable feature in \uppercase{bert} is the implementation of the WordPiece tokenizer module (\uppercase{\href{https://huggingface.co/course/chapter6/6?fw=pt}{quelle?}}).
Default BERT WordPiece tokenization is predominantly heuristic by combining strings based on a precalculated score.
A variety of pre-trained tokenizers are available, although they come with a caveat.
Once a tokenizer is trained on a dataset it is specific to that dataset.
This means the application of a tokenizer on another dataset may result in out-of-vocabulary issues and different token/subtoken distributions.

Particularly relevant to this thesis is the option to train an own tokenizer from the base module.
Usually, WordPiece generates its own set of subtokens called \textit{vocabulary}.
Tokens are then \uppercase{WORDPIECE algorithmus erklären}
By providing an algorithmically generated vocabulary to WordPiece and then training it on a new dataset the tokenization behavior is changed.




\subsection{Data}
\label{subsec:data}
explain the data that is used

\subsection{Benchmark}
\label{subsec:benchmark}
explain olmpics

\section{Implementation}
\label{sec:implementation}

Tatsächliche Anwendung der Methoden auf die Daten

\subsection{Tokenizer}
\label{subsec:tokenizer}
\uppercase{essentially deriving sensible subtokens to represent lexemes}

\subsubsection{Generating a custom pre-training vocabulary}
\label{subsubsec:generating-a-custom-pre-training-vocabulary}

The Wordmap algorithm as shown in \autoref{alg:wordmap} is the first step to extracting morphemes from a token.
Its purpose is to compare two strings and store their intersections in a map of boolean values.


\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\begin{algorithm}
    \caption{Wordmap generation}\label{alg:wordmap}
    \begin{algorithmic}[1]
        \Require $verbs = \{v : v \in C \wedge v_{\uppercase{POS}}\}, target$ \Comment{Set of single-\uppercase{POS} lexemic tokens}
        \Ensure $maps = (map_{1}, \ldots, map_{|verbs|})$
        \State $pair = (target, v)$
        \State $s = \textsc{shorter}(pair)$
        \State $l = \textsc{longer}(pair)$
        \State $case = \textsc{match\_ends}(pair)$ \Comment{Returns if strings match in the last or first position}
        \State $len = \textsc{len}(l)$
        \State $\delta = \Delta(len - \textsc{len}(s))$
        \\

        \Function{wordmap}{w1, w2, \*d=0}
            \State $f_{1}: c1, c2 \mapsto c1 == c2$
            \State $f_{2}: \left(\sum_{i=0+d}^{\mid w1 \mid} w1[i], w2[i] \mapsto f_{1}(w1[i], w2[i])\right)$
            \State \textbf{return} $f_{2}$
        \EndFunction
        \\
        \If{$\textit{case: any match}$}
            \If{$\delta$}
                \If{$\textit{case: left match}$}
                        \State $\textsc{wordmap}(l, s)$
                    \State Pad map from right side with 0s to match $len$
                \EndIf
                \If{$\textit{case: right match}$}
                        \State $\textsc{wordmap}(l, s, \delta)$
                    \State Pad map from left side with 0s to match $len$
                \EndIf
            \Else
                \State $\textsc{wordmap}(l, s)$
            \EndIf
        \EndIf

    \end{algorithmic}
\end{algorithm}

% DESCRIPTION OF ALGORITHM 1

Wordmap requires two \textbf{inputs} $verbs$ and $target$.
The resulting wordmap will be generated for $target$, while $verbs$ serves as comparison.
$verbs$ is a set of tokens pertaining to the same \ac{pos} category.
Note that $verbs$ should only contain those POS-tagged tokens that are expected to carry lexical information (e.g.\ verbs, adjectives, etc.).
The set is previously extracted from the corpus by \ac{pos}\hyphen tagging.
Optionally, the set can be augmented by manually adding \ac{pos} matching tokens from external sources.
The 2-tuple $pair$ are the strings to be compared.
It is passed on to (1) \textsc{shorter}, a function returning the shorter of both strings (2) \textsc{longer}, expectedly returning the longer of both strings (3) \textsc{match\_case}, a function to determine the behavior of the algorithm later on.
As two strings are compared \textsc{match\_case} captures three cases: $pair$ matches in the first, last or both positions.
Finally $len$ denotes the length of the longest string and $\delta$ difference in length between $s$ and $l$.
$\delta$ functions as an offset for index-based comparisons in \textsc{wordmap}.

\textsc{wordmap} is the function responsible for generating the wordmaps.


\begin{table}
    \centering
    \begin{tabular}{llll}
        \toprule
        \textbf{String} & \textbf{Wordmap} & \textbf{Case} & \textbf{Padding} \\
        \midrule
        \texttt{verarbeiten} & 111000000 & left match & Yes \\
         & 001000011 & right match & Yes \\
        \texttt{variiert} & 101001000 & left match & Yes \\
        \texttt{vormachen} & 101000111 & left match & No \\
        & 101000111 & right match & No \\
        \texttt{anstrebtest} &  & no match &   \\
        (...) &  &  &  \\
        \bottomrule
    \end{tabular}
    \caption{Example wordmaps for $target = \texttt{verstehen}$}
    \label{tab:}
\end{table}

Once every $v$ has been compared to $target$, $maps$ boolean counts of characters occurring in their respective positions in $target$.
Every map is cleaned with a regular expression to reduce noice caused by natural character occurrence.
Continuous concatenations of leading or trailing matches stay, while every match with the word enclosed by $0$ will be replaced the latter.
As an example, $wordmap = 11101100101$ contains three matches on the inside which will result in $11100000001$ as final output.
In the penultimate stage of mapping $target$, all maps are summed up to receive the number of absolute positional occurrences of every character in $target$.
This positional mapping allows for detecting relevant segments in a token based on a threshold.
Characters in range of the predefined threshold are selected for the mapping of a target token.
Functional morphemes (morphemes that are carriers of grammatical features) are typically much more frequent than their lexical counterparts.
Consequently, lexical morphemes in the family of inflectional languages are - by definition - modified by functional morphemes, they occur much less frequently.
In this case, the activation function for a concatenation of same boolean values to be selected as segment is the normalizing z-score function defined as: $z_{i} = \frac{x_{i} - \overline{x}}{S}$,
where $z$ is the z-score, $\overline{x}$ and $S$ are the sample mean and the sample standard deviation.


\subsubsection{BERT Tokenizer Modification}
\label{subsubsec:tokenizer-modification}
What is the model tokenizer mod
Which problems arose? What went well? What happened?



\subsection{Masked Language Model}
\label{subsec:masked-language-model}
Model implementation and parameters, runtimes?

\subsection{oLMpics Benchmark}
\label{subsec:olmpics-benchmark}

\begin{algorithm}
    \caption{Target Segmentation}\label{alg:segmenter}
    \begin{algorithmic}[2]
        \Require $target,pos\_vocab$ \Comment{short comment}
        \Ensure $\{(tuples\:of\:subwords)\} \approx \{ t \in \mathcal{P} (target \cap pos\_vocab) : t \equiv target\}$
        \\
        \State $segmentations = ()$
        \\

        \Function{segmenter}{token, stop, start=0, segments}
        \If{$start = stop$}
            \State Add $segments$ to $segmentations$
        \Else
            \State $morphemes = (m : m \in pos\_vocab \wedge target \textsc{startswith}(m))$
            \For{$m \in morphemes$}
                \State $start += \textsc{len}(m)$
                \State Add $m$ to $segments$
                \State $rest = target[\textsc{len}(m):]$
                \If{$\textsc{len}(rest)==1$}
                    \State $start = stop$
                    \State Add $rest$ to $segments$
                    \State Add $segments$ to $segmentations$
                    \State Decrement $start$, crop $segments$
                \Else
                    \State $\textsc{segmenter}(target=rest, stop, start=start, segments)$
                    \State Decrement $start$, crop $segments$
                \EndIf
            \EndFor
        \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}


tweak des tokenizers: segmentation ist eine frage der interpretation.
The
Ultimately, segmentation is a matter of interpretation.
As mentioned in~\ref{subsec:mlm}, the default WordPiece Tokenizer lacks
A linguistically informed



The field of NLP  (\cite{METZLER2016}) has been expanded ever since the emergence of the language models.
Natural language processing is understood as the