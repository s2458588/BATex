in this section the whole methodoloy is covered. what do i use in this thesis, why do i use it and lastly, how?
make sure the why covers methodological implications. (vergiss nicht alle pakete als quelle im Anhang)

\section{Requirements}
\label{sec:requirements}
A series of tools will help to achieve lexicalized tokenization.
They will be explained in this chapter along with their methodological edge.

% EVTL. MACHINE LEARNING MODEL AUFSPLITTEN IN MODEL, TOKENIZER VOCABULARY?
\subsection{Machine Learning Model}
\label{subsec:mlm}

\ac{bert} is a language learning transformer model designed for \ac{nlp} tasks (\cite{ATTENTION}).
Upon release it achieved higher performance scores compared to previously used \ac{lstm} models (\cite{BERTHIGH1}).
Two main model characteristics can be observed for \ac{bert}.
Firstly, it is the first \ac{lm} to implement simultaneous attention heads, allowing for bidirectional reading.
The methodological implication of reading to the left and right of a token is to include more information about the language in single embeddings.
Secondly, \ac{bert} introduced the (at the time novel) \ac{mlm} method for training.
The method involves masking a specified amount (default 15\%) of random tokens in the input sequence.
Masked tokens are guessed by the model which can then update its weights according to success or failure.

The \ac{nlp} community has since developed \ac{bert} and adapted it to the needs of contemporary \ac{nlp} problems (roberta, germanbert, mbert \uppercase{citation}).
Its wide support, comparability and versatility make \ac{bert} the model of choice for this thesis.
Another notable feature in \uppercase{bert} is the implementation of the WordPiece tokenizer module (\uppercase{\href{https://huggingface.co/course/chapter6/6?fw=pt}{quelle?}}).
Default BERT WordPiece tokenization is predominantly heuristic by combining strings based on a precalculated score.
A variety of pre-trained tokenizers are available, although they come with a caveat.
Once a tokenizer is trained on a dataset it is specific to that dataset.
This means the application of a tokenizer on another dataset may result in out-of-vocabulary issues and different token/subtoken distributions.

Particularly relevant to this thesis is the option to train an own tokenizer from the base module.
Usually, WordPiece generates its own set of subtokens called \textit{vocabulary}.
Tokens are then \uppercase{WORDPIECE algorithmus erklären}
By providing an algorithmically generated vocabulary to WordPiece and then training it on a new dataset the tokenization behavior is changed.




\subsection{Data}
\label{subsec:data}
explain the data that is used

\subsection{Benchmark}
\label{subsec:benchmark}
explain olmpics

\section{Implementation}
\label{sec:implementation}

Tatsächliche Anwendung der Methoden auf die Daten

\subsection{Tokenizer}
\label{subsec:tokenizer}
\uppercase{essentially deriving sensible subtokens to represent lexemes}

\subsubsection{Generating a custom pre-training vocabulary}
\label{subsubsec:generating-a-custom-pre-training-vocabulary}

Algorithm for the pre-training vocabulary:

\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\begin{algorithm}
    \caption{Generate wordmap}\label{alg:wordmap}
    \begin{algorithmic}
        \Require $verbs = \{v : v \in C \wedge v_{\uppercase{POS}}\}, target$ \Comment{Set of single-\uppercase{POS} lexemic tokens}
        \Ensure $maps = (map_{1}, \ldots, map_{|verbs|})$
        \State $pair = (target, v)$
        \State $case = \textsc{match\_ends}(pair)$ \Comment{Returns if strings match in the last or first position}
        \State $s = \textsc{shorter}(pair)$
        \State $l = \textsc{longer}(pair)$
        \State $len = \textsc{len}(l)$
        \State $\delta = \Delta(len - \textsc{len}(s))$
        \\

        \If{$\textit{any ends match}$}
            \If{$\delta$}
                \If{$\textit{case: left match}$}
                    \For{$i = 0$ \textbf{to} $len$}
                        \State $map[i] = f: (s[i], l[i]) \mapsto s[i] == l[i])$
                    \EndFor
                    \State Pad map from right side with 0s to match $\delta$
                \EndIf
                \If{$\textit{case: right match}$}
                    \For{$i = 0$ \textbf{to} $len$}
                        \State $map[i] = f: (s[i+\delta], l[i]) \mapsto s[i+\delta] == l[i])$
                    \EndFor
                    \State Pad map from left side with 0s to match $\delta$
                \EndIf
            \Else
                \State $map[i] = f: (s[i], l[i]) \mapsto s[i] == l[i])$
            \EndIf
        \EndIf

    \end{algorithmic}
\end{algorithm}

    \subsubsection{Tokenizer Training}
    \label{subsubsec:tokenizer-training}
    How did I train the tokenizer, how did it go?
    Which problems arose? What went well?

    \subsection{Masked Language Model}
    \label{subsec:masked-language-model}
    Model implementation and parameters, runtimes?

    \subsection{oLMpics Benchmark}
    \label{subsec:olmpics-benchmark}



    tweak des tokenizers: segmentation ist eine frage der interpretation.
    The
    Ultimately, segmentation is a matter of interpretation.
    As mentioned in~\ref{subsec:mlm}, the default WordPiece Tokenizer lacks
    A linguistically informed



    The field of NLP  (\cite{METZLER2016}) has been expanded ever since the emergence of the language models.
    Natural language processing is understood as the