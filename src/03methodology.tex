in this section the whole methodoloy is covered. what do i use in this thesis, why do i use it and lastly, how?
make sure the why covers methodological implications. (vergiss nicht alle pakete als quelle im Anhang)

\section{Requirements}
\label{sec:requirements}
A series of tools will help to achieve lexicalized tokenization.
They will be explained in this chapter along with their methodological edge.

% EVTL. MACHINE LEARNING MODEL AUFSPLITTEN IN MODEL, TOKENIZER VOCABULARY?
\subsection{Learning Architecture \& Tokenizer}
\label{subsec:mlm}

\ac{bert} is a language learning transformer model designed for \ac{nlp} tasks (\cite{ATTENTION}).
Upon release it achieved higher performance scores compared to previously used \ac{lstm} models (\cite{BERTHIGH1}).
Two main model characteristics can be observed for \ac{bert}.
Firstly, it is the first \ac{lm} to implement simultaneous attention heads, allowing for bidirectional reading.
The methodological implication of reading to the left and right of a token is to include more information about the language in single embeddings.
Secondly, \ac{bert} introduced the (at the time novel) \ac{mlm} method for training.
The method involves masking a specified amount (default 15\%) of random tokens in the input sequence.
Masked tokens are guessed by the model which can then update its weights according to success or failure.

The \ac{nlp} community has since developed \ac{bert} and adapted it to the needs of contemporary \ac{nlp} problems (roberta, germanbert, mbert \uppercase{citation}).
Its wide support, comparability and versatility make \ac{bert} the model of choice for this thesis.
Another notable feature in \uppercase{bert} is the implementation of the WordPiece tokenizer module (\uppercase{\href{https://huggingface.co/course/chapter6/6?fw=pt}{quelle?}}).
Default BERT WordPiece tokenization is predominantly heuristic by combining strings based on a precalculated score.
A variety of pre-trained tokenizers are available, although they come with a caveat.
Once a tokenizer is trained on a dataset it is specific to that dataset.
This means the application of a tokenizer on another dataset may result in out-of-vocabulary issues and different token/subtoken distributions.

Particularly relevant to this thesis is the option to train an own tokenizer from the base module.
Usually, WordPiece generates its own set of subtokens called \textit{vocabulary}.
Tokens are then \uppercase{WORDPIECE algorithmus erklären}
By providing an algorithmically generated vocabulary to WordPiece and then training it on a new dataset the tokenization behavior is changed.




\subsection{Data}
\label{subsec:data}
explain the data that is used
wiktionary verbs
hanta-crawled verbs of gerparcor
oscar de
gerparcor bundestag subset

\subsection{Benchmark}
\label{subsec:benchmark}
explain olmpics

\section{Implementation}
\label{sec:implementation}

Tatsächliche Anwendung der Methoden auf die Daten

\subsection{Tokenizer}
\label{subsec:tokenizer}
\uppercase{essentially deriving sensible subtokens to represent lexemes}
The methods of tokenization is explained in the following.
This is the main part of the thesis

Throughout this section, the words $target$ and $token$ are used to describe a word that is analyzed.
One denotes the argument of the wordmapping and segmenter function ($target$), the other describes a word occurring in the corpus ($token$).
\subsubsection{Generating a custom pre-training vocabulary}
\label{subsubsec:generating-a-custom-pre-training-vocabulary}

The Wordmap algorithm as shown in \autoref{alg:wordmap} is the first step to extracting morphemes from a token.
Its purpose is to compare two strings and store their intersections in a map of boolean values.


\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\begin{algorithm}
    \caption{Wordmap generation}\label{alg:wordmap}
    \begin{algorithmic}[1]
        \Require $verbs = \{v : v  v_{\uppercase{POS}}\}, target$ \Comment{Set of single-\uppercase{POS} lexemic tokens}
        \Ensure $maps = (map_{1}, \ldots, map_{|verbs|})$
        \Function{wordmap}{w1, w2, \*d=0}
            \State $f_{1}: c1, c2 \mapsto c1 == c2$
            \State $f_{2}: \left(\sum_{i=0+d}^{\mid w1 \mid} w1[i], w2[i] \mapsto f_{1}(w1[i], w2[i])\right)$
            \State \textbf{return} $f_{2}$
        \EndFunction
        \\

        \For{$v \in verbs$}
        \State $pair = (target, v)$
        \State $s = \textsc{shorter}(pair)$
        \State $l = \textsc{longer}(pair)$
        \State $case = \textsc{match\_ends}(pair)$ \Comment{Returns if strings match in the last or first position}
        \State $len = \textsc{len}(l)$
        \State $\delta = \Delta(len, \textsc{len}(s))$
        \\


        
        \If{$\textit{case: any match}$}
            \If{$\delta$}
                \If{$\textit{case: left match}$}
                        \State $\textsc{wordmap}(l, s)$
                    \State Pad map from right side with 0s to match $len$
                \EndIf
                \If{$\textit{case: right match}$}
                        \State $\textsc{wordmap}(l, s, \delta)$
                    \State Pad map from left side with 0s to match $len$
                \EndIf
            \Else
                \State $\textsc{wordmap}(l, s)$
            \EndIf
        \EndIf
        \EndFor

    \end{algorithmic}
\end{algorithm}

% DESCRIPTION OF ALGORITHM 1

Wordmap requires two \textbf{inputs} $verbs$ and $target$.
The resulting wordmap will be generated for $target$, while $verbs$ serves as comparison.
Any map generated from this also has the same length as $target$.
$verbs$ is a set of tokens pertaining to the same \ac{pos} category.
Note that $verbs$ should only contain those POS-tagged tokens that are expected to carry lexical information (e.g.\ verbs, adjectives, etc.).
The set is previously extracted from the corpus by \ac{pos}\hyphen tagging.
Optionally, the set can be augmented by manually adding \ac{pos} matching tokens from external sources.
The 2-tuple $pair$ are the strings to be compared.
It is passed on to (1) \textsc{shorter}, a function returning the shorter of both strings (2) \textsc{longer}, expectedly returning the longer of both strings (3) \textsc{match\_case}, a function to determine the behavior of the algorithm later on.
As two strings are compared \textsc{match\_case} captures three cases: $pair$ matches in the first, last or both positions.
Finally $len$ denotes the length of the longest string and $\delta$ difference in length between $s$ and $l$.
$\delta$ functions as an offset for index-based comparisons \textsc{wordmap}.

\textsc{wordmap} is a naive mapping function generating the wordmaps.


\begin{table}
    \centering
    \begin{tabular}{llll}
        \toprule
        \textbf{String} & \textbf{Wordmap} & \textbf{Case} & \textbf{Padding} \\
        \midrule
        \texttt{verarbeiten} & 111000000 & left match & Yes \\
         & 001000011 & right match & Yes \\
        \texttt{variiert} & 101001000 & left match & Yes \\
        \texttt{vormachen} & 101000111 & left match & No \\
        & 101000111 & right match & No \\
        \texttt{anstrebtest} &  & no match &   \\
        (...) &  &  &  \\
        \bottomrule
    \end{tabular}
    \caption{Example wordmaps for $target = \texttt{verstehen}$}
    \label{tab:}
\end{table}

Once every $v$ has been compared to $target$, $maps$ boolean counts of characters occurring in their respective positions in $target$.
Every map is cleaned with a regular expression to reduce noice caused by natural character occurrence.
Continuous concatenations of leading or trailing matches stay, while every match with the word enclosed by $0$ will be replaced the latter.
As an example, $wordmap = 11101100101$ contains three matches on the inside which will result in $11100000001$ as final output.
In the penultimate stage of mapping $target$, all maps are summed up to receive the number of absolute positional occurrences of every character in $target$.
This positional mapping allows for detecting relevant segments in a token based on a threshold.
Characters in range of the predefined threshold are selected for the mapping of a target token.
Functional morphemes (morphemes that are carriers of grammatical features) are typically much more frequent than their lexical counterparts.
Consequently, lexical morphemes in the family of inflectional languages are - by definition - modified by functional morphemes, they occur much less frequently.
In this case, the activation function for a concatenation of same boolean values to be selected as segment is the normalizing z-score function defined as: $z_{i} = \frac{x_{i} - \overline{x}}{S}$,
where $z$ is the z-score, $\overline{x}$ and $S$ are the sample mean and the sample standard deviation.


\subsubsection{BERT Tokenizer Modification}
\label{subsubsec:tokenizer-modification}
What is the model tokenizer mod
Which problems arose? What went well? What happened?



\subsection{Masked Language Model}
\label{subsec:masked-language-model}
Model implementation and parameters, runtimes?

\subsection{oLMpics Benchmark}
\label{subsec:olmpics-benchmark}

\begin{algorithm}
    \caption{Target Segmentation}\label{alg:segmenter}
    \begin{algorithmic}[1]
        \Require $target,pos\_vocab$ \Comment{short comment}
        \Ensure $\{(tuples\:of\:subwords)\} \approx \{ t \in \mathcal{P} (s \in pos\_vocab : s \in target) : t \equiv target\}$
        \\
        \State $segmentations = ()$
        \\

        \Function{segmenter}{token, stop, start=0, segments}
        \If{$start = stop$}
            \State Add $segments$ to $segmentations$
        \Else
            \State $morphemes = (m \in pos\_vocab : target. \textsc{startswith}(m) \wedge |m|>1)$
            \For{$m \in morphemes$}
                \State $start \mathrel{+}= \textsc{len}(m)$
                \State Add $m$ to $segments$
                \State $rest = target[\textsc{len}(m):]$
                \If{$\textsc{len}(rest)==1$}
                    \State $start = stop$
                    \State Add $rest$ to $segments$
                    \State Add $segments$ to $segmentations$
                    \State Decrement $start$, crop $segments$
                \Else
                    \State $\textsc{segmenter}(target=rest, stop, start=start, segments)$
                    \State Decrement $start$, crop $segments$
                \EndIf
            \EndFor
        \EndIf
        \EndFunction
        \State $\textsc{maximize\_segments}(segmentations)$
    \end{algorithmic}
\end{algorithm}

\autoref{alg:segmenter} recursively computes every possible segmentation for a string $target$ from a given vocabulary $pos\_vocab$ from left to right.
The vocabulary contains all the segments that were identified in the previous step by \autoref{alg:wordmap}.
Every segmentation has to be complete so that its segments corresponds to non overlapping substrings of $target$.
For every $target$ a subvocabulary $morpheme$ is defined, containing all strings that are in the vocabulary of \ac{POS} \- members.
This task is a weighted coverage problem in the \uppercase{NP}-hard domain.
Unary morphemes to the left are excluded from the pre-selected subvocabulary to drastically reduce the number of possible permutations , as they can be embedded in n-ary tokens as well.
The recursion can be seen as n-ary a trees containing every permutation of the set $morphemes$ where the sum of branches all satisfy $target$.

First, all morphemes that $targets$ starts with are stored to form the first nodes of the permutation trees.
Each time a morpheme is selected the index $start$ is incremented by the length of the morpheme to indicate when the string has been completely segmented.
The new recursion is called with the updated index $start$ and $target$ sliced by the legnth of the morpheme contained in the parent node.
Incomplete segmentations that miss exactly one character to the right are accepted with the added missing string.
If the vocabulary cannot satisfy a segmentation by missing the necessary strings segmentation is omitted and the original input token is returned as such.


\begin{figure}\label{fig:segmentationtree}
\centering
\begin{forest}
    for tree={
        grow=south,
        minimum size=3ex, inner sep=1pt,
        s sep=7mm
    }
    [$\mathbb{P}$\{versteh, vers, ver, ve\}

    [\texttt{versteh}
    [\texttt{en}]
    ]
    [\texttt{vers}
    [\texttt{teh} [\texttt{en}]]
    [\texttt{te}
        \texttt{he}
        [\texttt{n}]]]

    [\texttt{ver}
    [\texttt{steh}
    [\texttt{en}]]
    [\texttt{ste}
    [\texttt{he} [\texttt{n}]]]
    [\texttt{st}
    [\texttt{ehe} [\texttt{n}]]
    [\texttt{eh} [\texttt{en}]]]
    ]
    [\texttt{ve}
    [\texttt{rsteh} [\texttt{en}]]
    [\texttt{rst}
    [\texttt{eh} [\texttt{en}]]
    [\texttt{ehe} [\texttt{n}]]
    ]
    [ \texttt{rste} [\texttt{he} [\texttt{n}]]]
    ]
    ]



\end{forest}
\end{figure}

Then, of all $segmentations$ a single segmentation is selected by a maximization function \textsc{maximize\_segments} calculating weights for every segment.
The maximization function for one segment is the defined as:

\begin{equation}\label{eq:maximization}
    \underset{s \in S}{\arg\max} f(x) \coloneqq  \lbrace \sum_{i=1}^{|s|} \sqrt[m]{\frac{s_{i}}{t} \div |s|}\rbrace
\end{equation}

Where $s$ is a segmentation tuple, $|s|$ is the length of the tuple (read: number of segments) and $s_{i}$ indicates the segment of the tuple at position $i$.
For every segment (vertex) in a segmentation tuple (branch)  the segment's coverage length is divided by the token length $t$ to get the coverage of the morpheme


tweak des tokenizers: segmentation ist eine frage der interpretation.
The
Ultimately, segmentation is a matter of interpretation.
If there are several possible interpretations by wich to segment a word this tokenization method relies on the assumption that the comparison with \ac{pos}\hypen members displays the probable shape of segmentation for a token.
As mentioned in~\ref{subsec:mlm}, the default WordPiece Tokenizer lacks
A linguistically informed



The field of NLP  (\cite{METZLER2016}) has been expanded ever since the emergence of the language models.
Natural language processing is understood as the