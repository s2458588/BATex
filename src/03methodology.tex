in this section the whole methodoloy is covered.
what do i use in this thesis, why do i use it and lastly, how?
make sure the why covers methodological implications.
(vergiss nicht alle pakete als quelle im Anhang)

\section{Requirements}
\label{sec:requirements}
A series of tools will help to achieve lexicalized tokenization.
They will be explained in this chapter along with their methodological edge.

% EVTL. MACHINE LEARNING MODEL AUFSPLITTEN IN MODEL, TOKENIZER VOCABULARY?
\subsection{Pipeline Structure \& Transformers Library}
\label{subsec:architecture}

\ac{bert} is a language learning transformer model designed for \ac{nlp} tasks (\cite{ATTENTION}).
Upon release it achieved higher performance scores compared to previously used \ac{lstm} models (\cite{BERTHIGH1}).
Two main model characteristics can be observed for \ac{bert}.
Firstly, it is the first \ac{lm} to implement simultaneous attention heads, allowing for bidirectional reading.
The methodological implication of reading to the left and right of a token is to include more information about the language in single embeddings.
Secondly, \ac{bert} introduced the (at the time novel) \ac{mlm} method for training.
The method involves masking a specified amount (default 15\%) of random tokens in the input sequence.
Masked tokens are guessed by the model which can then update its weights according to success or failure.

The \ac{nlp} community has since developed \ac{bert} and adapted it to the needs of contemporary \ac{nlp} problems (roberta, germanbert, mbert \uppercase{citation}).
Its wide support, comparability and versatility make \ac{bert} the model of choice for this thesis.
Another notable feature in \uppercase{bert} is the implementation of the WordPiece tokenizer module (\uppercase{\href{https://huggingface.co/course/chapter6/6?fw=pt}{quelle?}}).
Default BERT WordPiece tokenization is predominantly heuristic by combining strings based on a precalculated score.
A variety of pre-trained tokenizers are available, although they come with a caveat.
Once a tokenizer is trained on a dataset it is specific to that dataset.
This means the application of a tokenizer on another dataset may result in out-of-vocabulary issues and different token/subtoken distributions.

Particularly relevant to this thesis is the option to train an own tokenizer from the base module.
Usually, WordPiece generates its own set of subtokens called \textit{vocabulary}, which is used by the standard tokenizer to generate unique input numerical IDs.
These identifiers correspond to tokens or parts of tokens found in the original dataset that the model was trained on.
Wordmap partly takes over the tokenization depending on the the type of input it receives, the exact process is given in \autoref{subsec:tokenizer}.
Once a string is tokenized it gets passed on to the transformer model for contextualized embedding.
Schematic \autoref{fig:pipeline} shows the processing of data up until token encoding.

\begin{figure}
    \centering

    \tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt

    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
        \draw   (40,98) .. controls (40,93.58) and (43.58,90) .. (48,90) -- (172,90) .. controls (176.42,90) and (180,93.58) .. (180,98) -- (180,130) .. controls (180,130) and (180,130) .. (180,130) -- (40,130) .. controls (40,130) and (40,130) .. (40,130) -- cycle ;
        \draw   (210,98) .. controls (210,93.58) and (213.58,90) .. (218,90) -- (352,90) .. controls (356.42,90) and (360,93.58) .. (360,98) -- (360,130) .. controls (360,130) and (360,130) .. (360,130) -- (210,130) .. controls (210,130) and (210,130) .. (210,130) -- cycle ;
        \draw   (210,150) -- (360,150) -- (360,210) -- (210,210) -- cycle ;
        \draw   (210,230) -- (360,230) -- (360,270) -- (210,270) -- cycle ;
        \draw   (40,150) -- (180,150) -- (180,270) -- (40,270) -- cycle ;
        \draw   (40,290) -- (180,290) -- (180,340) -- (40,340) -- cycle ;
        \draw   (210,290) -- (360,290) -- (360,340) -- (210,340) -- cycle ;
        \draw   (210,360) -- (360,360) -- (360,400) -- (210,400) -- cycle ;
        \draw [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] [dash pattern={on 0.84pt off 2.51pt}]  (390,110) -- (362,110) ;
        \draw [shift={(360,110)}, rotate = 360] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (12,-3) -- (0,0) -- (12,3) -- cycle    ;
        \draw    (285,130) -- (285,147) ;
        \draw [shift={(285,150)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
        \draw    (110,130) -- (110,147) ;
        \draw [shift={(110,150)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
        \draw    (110,270) -- (110,287) ;
        \draw [shift={(110,290)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
        \draw    (285,210) -- (285,227) ;
        \draw [shift={(285,230)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
        \draw    (285,270) -- (285,287) ;
        \draw [shift={(285,290)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
        \draw    (285,340) -- (285,357) ;
        \draw [shift={(285,360)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
        \draw    (180,310) -- (207,310) ;
        \draw [shift={(210,310)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
        \draw [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] [dash pattern={on 0.84pt off 2.51pt}]  (180,110) -- (208,110) ;
        \draw [shift={(210,110)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (12,-3) -- (0,0) -- (12,3) -- cycle    ;
        \draw   (40,360) -- (180,360) -- (180,400) -- (40,400) -- cycle ;
        \draw    (210,380) -- (183,380) ;
        \draw [shift={(180,380)}, rotate = 360] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
        \draw [fill={rgb, 255:red, 65; green, 183; blue, 159 }  ,fill opacity=0.79 ]   (110,400) -- (110,417) ;
        \draw [shift={(110,420)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
        \draw  [dash pattern={on 0.84pt off 2.51pt}]  (390,250) -- (390,380) ;
        \draw [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] [dash pattern={on 0.84pt off 2.51pt}]  (390,380) -- (362,380) ;
        \draw [shift={(360,380)}, rotate = 360] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (12,-3) -- (0,0) -- (12,3) -- cycle    ;
        \draw [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=1 ] [dash pattern={on 0.84pt off 2.51pt}]  (390,250) -- (360,250) ;
        \draw (111.6,209) node   [align=left] {\begin{minipage}[lt]{83.78pt}\setlength\topsep{0pt}
        \begin{center}
            \textbf{Datasets}\\GerParCor (500k)\\\&\\Oscar (500k)
        \end{center}
        \end{minipage}};
        \draw (113.27,110.5) node   [align=left] {\textbf{Datafiles}};
        \draw (284.19,110.5) node   [align=left] {\textbf{POS Datafiles}};
        \draw (283.62,180) node   [align=left] {\begin{minipage}[lt]{55.24pt}\setlength\topsep{0pt}
        \begin{center}
            \textbf{Wordmaps}
        \end{center}
        Generation
        \end{minipage}};
        \draw (284.8,250.5) node   [align=left] {\textbf{POS Vocabulary}};
        \draw (286.7,316) node   [align=left] {\begin{minipage}[lt]{88.34pt}\setlength\topsep{0pt}
        \begin{center}
            \textbf{WMTokenizer}\\POS segmenting
        \end{center}
        \end{minipage}};
        \draw (286.87,380.5) node   [align=left] {\textbf{BERT Tokenizer}};
        \draw (111.37,316) node   [align=left] {\begin{minipage}[lt]{56.94pt}\setlength\topsep{0pt}
        \begin{center}
            \textbf{HanTa}\\ i/o
        \end{center}
        \end{minipage}};
        \draw (110.34,380.5) node   [align=left] {\textbf{Encodings}};

    \end{tikzpicture}
    \caption{Schematic integration of Wordmap segmentation into the BERT architecture. Continuous lines show non-selective channels, while dotted lines only pass data selectively}
    \label{fig:pipeline}
\end{figure}

In \autoref{fig:pipeline}, Datafiles contains the raw unprocessed data coming from corpora.
The input is provided in txt-files holding sentences line by line.
Two datasets are generated from this input, each amounting to 500000 sentences per corpus for fine-tuning.
POS Datafiles ideally contains only words that are identified as the same \ac{pos} category.
To increase the resulting POS vocabulary, POS Datafiles can be augmented by two channels: input can come from the corpus itself, but would need to be tagged first.
Otherwise a predefined list of words can be used as an external and generic source for Wordmap generation.
In POS Vocabulary, naive adjustments like removing outliers and interferences are performed on the set of subwords yielded by Wordmap generation.
It is important to note that the POS vocabulary is separate from the the tokenizer vocabulary.
This serves the specific purpose of keeping the vocabulary on which the model is trained clean from unwanted or unused subwords.
Machine learning practice generally points to the trade-off between vocabulary size and model performance, hence the addition of POS Vocabulary to the BERT Tokenizer vocabulary is used only when necessary.
Datasets are then passed through \ac{hanta} to provide WMTokenizer with flagged data to tokenize only selected tokens.
Lastly BERT Tokenizer returns the canonical encodings known from language modeling.

\subsection{Data}
\label{subsec:data}
Two corpora where selected for fine-tuning: \ac{gerparcor} and oscar (\cite{oscar}).
FLOTA was trained on 12000 samples per  category ~\cite{FLOTA}, which is why for this fine-tuning a sample size of 500000 seems sufficient.
GerParCor is a \enquote{GerParCor genre-specific corpus of (predominantly historical) German-language parliamentary protocols from three centuries and four countries, including state and federal level data.} (\cite[1]{GERPARCOR}).
Of all subcorpora included in \ac{gerparcor}, one specifically texts and transcripts of the german parliament.
All 500k samples in the \ac{gerparcor} dataset used for this thesis is found in the \texttt{Bundestag} subcorpus of \ac{gerparcor}.
Oscar offers other subcorpora partly hosted on huggingface.
In this case, a web-crawled german corpus called \texttt{unshuffled\_deduplicated\_de} was chosen, as introduced by its curators \textcite{oscarsub2}; (\citeyear{oscarsub1}).

The POS vocabulary comes from ~\cite{wiktionary}
hanta-crawled verbs of gerparcor ~\cite{hanta}


\subsection{Benchmark}
\label{subsec:benchmark}
The benchmark used on the model is oLMpics (\cite{olmpics})

\section{Implementation}
\label{sec:implementation}

TatsÃ¤chliche Anwendung der Methoden auf die Daten

\subsection{Tokenizer}
\label{subsec:tokenizer}
\uppercase{essentially deriving sensible subtokens to represent lexemes}
The methods of tokenization is explained in the following.
This is the main part of the thesis containing two algorithms for vocabulary generation and token segmentation.
Once the two algorithms have been explained, the last section of this chapter features the


Languages, as consistent form of communication, always
Throughout this section, the words $target$ and $token$ are used to describe a word that is analyzed.
One denotes the argument of the wordmapping and segmenter function ($target$), the other describes a word occurring in the corpus ($token$).

\subsubsection{Generating a custom pre-training vocabulary}
\label{subsubsec:generating-a-custom-pre-training-vocabulary}

Embedding those subwords which take part in inflectional processes essentially means deriving sensible subwords to represent actual morphemes.

The vocabulary generated for this chapter is a list of inflected and non-inflected verbs (as to state the example) provided by two sources: (1) the german verb wiktionary and (2) a crawl of the GerParCor corpus with the HanTa tagger \cite{hanta}.
Initial experiments where done only with the wiktionary verb list since it provided sanitized input for the algorithm in development.
As soon as the verbs where extracted from \ac{gerparcor} via \ac{hanta}

The Wordmap algorithm as shown in \autoref{alg:wordmap} is the first step to extracting morphemes from a token.
Its purpose is to compare two strings and store their intersections in a map of boolean values.


\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\begin{algorithm}
    \caption{Wordmap generation}\label{alg:wordmap}
    \begin{algorithmic}[1]
        \Require $verbs = \{v : \uppercase{POS}\}, target$ \Comment{\textit{verbs}: set of single-\uppercase{POS} lexemic tokens}
        \Ensure $maps = (map_{1}, \ldots, map_{|verbs|})$
        \Function{wordmap}{w1, w2, \*d=0}
            \State $f_{1}: c1, c2 \mapsto c1 == c2$
            \State $f_{2}: \left(\sum_{i=0+d}^{\mid w1 \mid} w1[i], w2[i] \mapsto f_{1}(w1[i], w2[i])\right)$
            \State \textbf{return} $f_{2}$
        \EndFunction
        \\

        \For{$v \in verbs$}
        \State $pair = (target, v)$
        \State $s = \textsc{shorter}(pair)$
        \State $l = \textsc{longer}(pair)$
        \State $case = \textsc{match\_ends}(pair)$ \Comment{Returns if strings match in the last or first position}
        \State $len = \textsc{len}(l)$
        \State $\delta = \Delta(len, \textsc{len}(s))$
        \\

        \If{$\textit{case: any match}$}
            \If{$\delta$}
                \If{$\textit{case: left match}$}
                        \State $\textsc{wordmap}(l, s)$
                    \State Pad map from right side with 0s to match $len$
                \EndIf
                \If{$\textit{case: right match}$}
                        \State $\textsc{wordmap}(l, s, \delta)$
                    \State Pad map from left side with 0s to match $len$
                \EndIf
            \Else
                \State $\textsc{wordmap}(l, s)$
            \EndIf
        \EndIf
        \EndFor

    \end{algorithmic}
\end{algorithm}

% DESCRIPTION OF ALGORITHM 1

Wordmap requires two \textbf{inputs} $verbs$ and $target$.
The resulting wordmap will be generated for $target$, while $verbs$ serves as comparison.
Any map generated from this also has the same length as $target$.
$verbs$ is a set of tokens pertaining to the same \ac{pos} category.
Note that $verbs$ should only contain those POS-tagged tokens that are expected to carry lexical information (e.g.\ verbs, adjectives, etc.).
The set is previously extracted from the corpus by \ac{pos}\hyphen tagging.
Optionally, the set can be augmented by manually adding \ac{pos} matching tokens from external sources.
The 2-tuple $pair$ are the strings to be compared.
It is passed on to (1) \textsc{shorter}, a function returning the shorter of both strings (2) \textsc{longer}, expectedly returning the longer of both strings (3) \textsc{match\_case}, a function to determine the behavior of the algorithm later on.
As two strings are compared \textsc{match\_case} captures three cases: $pair$ matches in the first, last or both positions.
Finally $len$ denotes the length of the longest string and $\delta$ difference in length between $s$ and $l$.
$\delta$ functions as an offset for index-based comparisons \textsc{wordmap}.

\textsc{wordmap} is a naive mapping function generating the wordmaps.


\begin{table}
    \centering
    \caption{Example wordmaps for $target = \texttt{verstehen}$}
    \label{tab:wordmaps}
    \begin{tabular}{llll}
        \toprule
        \textbf{String} & \textbf{Wordmap} & \textbf{Case} & \textbf{Padding} \\
        \midrule
        \texttt{verarbeiten} & 111000000 & left match & Yes \\
         & 001000011 & right match & Yes \\
        \texttt{variiert} & 101001000 & left match & Yes \\
        \texttt{vormachen} & 101000111 & left match & No \\
        & 101000111 & right match & No \\
        \texttt{anstrebtest} &  & no match &   \\
        (...) &  &  &  \\
        \bottomrule
    \end{tabular}
\end{table}

Once every $v$ has been compared to $target$, $maps$ stores boolean counts of characters occurring in their respective positions in $target$.
Every map is cleaned with a regular expression to reduce noise caused by natural character occurrence (some characters like \textless n\textgreater, \textless s\textgreater ~will be more frequent than others).
Continuous concatenations of leading or trailing matches stay, while every match enclosed by $0$ will be replaced the $0$.
As an example, $wordmap = 11101100101$ contains three matches on the inside which will result in $11100000001$ as final output.
In the penultimate stage of mapping $target$, all maps are summed up to receive the number of absolute positional occurrences of every character in $target$.
This positional mapping allows for detecting relevant segments in a token based on a threshold.
Characters in range of the predefined threshold are selected for the mapping of a target token.
Functional morphemes (morphemes that are carriers of grammatical features) are typically much more frequent than their lexical counterparts.
Consequently, lexical morphemes in the family of inflectional languages are - by definition - modified by functional morphemes, they occur much less frequently.
In this case, the activation function for a concatenation of same boolean values to be selected as segment is the normalizing z-score function defined as: $z_{i} = \frac{x_{i} - \overline{x}}{S}$,
where $z$ is the z-score, $\overline{x}$ and $S$ are the sample mean and the sample standard deviation.

%TODO: this makes it robust against spelling errors because they cant durchsetz

\subsubsection{BERT Tokenizer Modification}
\label{subsubsec:tokenizer-modification}


\begin{algorithm}
    \caption{Target Segmentation}\label{alg:segmenter}
    \begin{algorithmic}[1]
        \Require $target,pos\_vocab$ \Comment{Vocabulary consisting of verbs only}
        \Ensure $\{(tuples\:of\:subwords)\} \approx \{ t \in \mathcal{P} (s \in pos\_vocab : s \in target) : t \equiv target\}$
        \\
        \State $segmentations = ()$
        \\

        \Function{segmenter}{token, stop, start=0, segments}
        \If{$start = stop$}
            \State Add $segments$ to $segmentations$
        \Else
            \State $morphemes = (m \in pos\_vocab : target. \textsc{startswith}(m) \wedge |m|>1)$
            \For{$m \in morphemes$}
                \State $start \mathrel{+}= \textsc{len}(m)$
                \State Add $m$ to $segments$
                \State $rest = target[\textsc{len}(m):]$
                \If{$\textsc{len}(rest)==1$}
                    \State $start = stop$
                    \State Add $rest$ to $segments$
                    \State Add $segments$ to $segmentations$
                    \State Decrement $start$, crop $segments$
                \Else
                    \State $\textsc{segmenter}(target=rest, stop, start=start, segments)$
                    \State Decrement $start$, crop $segments$
                \EndIf
            \EndFor
        \EndIf
        \EndFunction
        \State $\textsc{maximize\_segments}(segmentations)$
    \end{algorithmic}
\end{algorithm}

\autoref{alg:segmenter} recursively computes every possible segmentation for a string $target$ from a given vocabulary $pos\_vocab$ from left to right.
The vocabulary contains all the segments that were identified in the previous step by \autoref{alg:wordmap}.
Every segmentation has to be complete so that its segments corresponds to non overlapping substrings of $target$.
For every $target$ a subvocabulary $morpheme$ is defined, containing all strings that are in the vocabulary of \ac{POS} \- members.
This task is a weighted coverage problem in the \uppercase{NP}-hard domain.
Unary morphemes to the left are excluded from the pre-selected subvocabulary to drastically reduce the number of possible permutations , as they can be embedded in n-ary tokens as well.
The recursion can be seen as n-ary a trees containing every permutation of the set $morphemes$ where the sum of branches all satisfy $target$.

\begin{figure}
\centering
\begin{forest}
    for tree={
        grow=south,
        minimum size=2ex, inner sep=1pt,
        s sep=7mm
    }
    [Call: & \textsc{segmenter}(\texttt{"verstehen"})\\\hline
    Subvocab: & \{\texttt{versteh\, vers\, ver\, ve\, en\, teh\, \, steh\, ste\, st\, rsteh\, rst\, rste\, ehe\, he\, n\}}\\
    ,align=11,draw
    [{\texttt{verstehen}}
    [\texttt{versteh}
    [\texttt{en}]
    ]
    [\texttt{vers}
    [\texttt{teh}
    [\texttt{en}]]
    [\texttt{te}
        [\texttt{he}
        [\texttt{n}]]]]

    [\texttt{ver}, draw, dotted
    [\texttt{steh}, draw, dotted
    [\texttt{en}, draw, dotted
    ]
    ]
    [\texttt{ste}
    [\texttt{he} [\texttt{n}]]]
    [\texttt{st}
    [\texttt{ehe} [\texttt{n}]]
    [\texttt{eh} [\texttt{en}]]]
    ]
    [\texttt{ve}
    [\texttt{rsteh} [\texttt{en}]]
    [\texttt{rst}
    [\texttt{eh} [\texttt{en}]]
    [\texttt{ehe} [\texttt{n}]]
    ]
    [ \texttt{rste} [\texttt{he} [\texttt{n}]]]
    ]
    ]
    ]


\end{forest}
\caption{Permutations \textsc{segmenter} generates from target i.e. \texttt{verstehen}. Dotted segments mark the segmentation chosen later by the maximization function during tokenization.}
\label{fig:segmentationtree}
\end{figure}

As shown in \autoref{fig:segmentationtree}, all morphemes that $target$ starts with are stored to form the first nodes of the permutation tree.
Each time a morpheme is selected the index $start$ is incremented by the length of the morpheme to indicate when the string has been completely segmented.
The new recursion is called with the updated index $start$ and $target$ sliced by the length of the morpheme contained in the parent node.
Incomplete segmentations that miss exactly one character to the right are accepted with the added missing string.
If the vocabulary cannot satisfy a segmentation because it is missing the necessary strings, segmentation is omitted and the original input token is returned as such.

Then, of all $segmentations$ a single segmentation is selected by a maximization function \textsc{maximize\_segments} calculating weights for every segment.
The maximization function for one segment is the defined as:

\begin{equation}\label{eq:maximization}
    \underset{s \in S}{\arg\max} f(s) \coloneqq  \Bigl\{ s \in S : \sum_{i=1}^{|s|} \sqrt[s_{i}]{\frac{s_{i}}{t} \div |s|}\Bigr\}
\end{equation}

Where $S$ is a set of segmentation tuples $s$, $|s|$ is the length of the tuple (read: number of segments) and $s_{i}$ indicates the length of the segment at position $i$ in tuple $s$.
For every segment (vertex) in a segmentation tuple (branch)  the segment's length is divided by the token length $t$ to get the coverage the morpheme provides towards the target string.
The number of segments $|s|$ is a divisor meant to cap the number of segments in a segmentation.
It prevents choosing a segmentation overflowing with too many short segments.
Short segments are convenient for completing a segmentation, but will increase the chance of slicing a token where it linguistically lacks sense to do so.
Lastly the $s_{i}$^{\text{th}} root implements a bias against segments that are too long.
While the accuracy decreases for longer words, this method of maximization performs reasonably well in the range of 1--4 syllable tokens, which make up around 87\% of the verb vocabulary.


Ultimately, segmentation is a matter of interpretation.
If there are several possible interpretations by wich to segment a word this tokenization method relies on the assumption that the comparison with \ac{pos}- members displays the probable shape of segmentation for a token.
As mentioned in~\ref{subsec:masked-language-model}, the default WordPiece Tokenizer lacks
A linguistically informed



The field of NLP  (\cite{METZLER2016}) has been expanded ever since the emergence of the language models.
Natural language processing is understood as the



\subsection{Masked Language Model}
\label{subsec:masked-language-model}
Model implementation and parameters, runtimes?

\subsection{oLMpics Benchmark}
\label{subsec:olmpics-benchmark}