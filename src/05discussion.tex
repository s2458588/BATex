
algorithm doesnt see circumfixes like ge---t) but only affixes, this could be possible with better noise control and map interpolation

Characterizing the functional and lexical morphemes by their respective average length has had a positibe impact on segmentation, seem to have impacted the models performance very much.

it is hard to tell if the results are good without combining further metrics like loss or confusion matrix

"A low precision score (<0.5) means your classifier has a high number of False positives which can be an outcome of imbalanced class or untuned model hyperparameters. In an imbalanced class problem"
-> unbalanced sampling



spelling errors: oscar > gpc (effekt?)

learning the set != better performance (standard different learning, similar performance)
mw test scores > ms test scores (in precision + recall, but not in f1 -> imbalance?)

jitter due to small sample size may distort how conclusive the final scores are