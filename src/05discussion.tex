Method: tokenizer threshold.
Interestingly, there are many segment thresholds to choose from

there should be a filter to mitigate the natural distribution of common characters like s, n to be detected as  part of a morpheme

whole integrated system with overhead pos for all lexical categories
scaled with the degree of fusion and inflection that language has

algorithm doesnt see circumfixes like ge---t) but only affixes, this could be possible with better noise control and map interpolation

Characterizing the functional and lexical morphemes by their respective average length has had a positibe impact on segmentation, seem to have impacted the models performance very much.

it is hard to tell if the results are good without combining further metrics like loss or confusion matrix

"A low precision score (<0.5) means your classifier has a high number of False positives which can be an outcome of imbalanced class or untuned model hyperparameters. In an imbalanced class problem"
-> unbalanced sampling
