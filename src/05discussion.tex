Results of the pre-trained model benchmarks and tokenization are analyzed in this chapter.
A closer look is taken at the training procedure and the benchmark results to identify possible effects in data or model architecture.
For purposes of readability the models are referred to by their IDs introduced in \autoref{subsec:masked-language-model}.\\

The first apparent difference when comparing the models presented in this study are the test scores.
An overall sub-par performance can be observed for the non-ggbc models, while their baseline model clearly performed better at the task.
There are many causes for low model performance.
A model runs through many direct (hyperparameters, implementation) or indirect (data quality, task choice) modeling steps.
For the course of this thesis a smaller set of experiment parameters where chosen to establish a controlled environment for hypothesis testing.
Relating to the models this includes the following design measures: (1) smaller model and sample size, (2) constructing Wordmap tokenization and using it on singular tokens during batching, and (3) a single evaluation task.
Since the larger model performed better than the smaller ones fine-tuned on Oscar and GerParCor one could assume that the performance was influenced by the sheer size of input data and model parameters.
However, there are enough accounts for outstanding performances in current machine learning applications for smaller models (\textcite{smallmodelbigbang}; \textcite{fewshotTHREEpercent}).
\citeauthor{scalinglaws} provide an extensive study of model scaling stating that while larger models tend to be more sample-efficient, any of the factors computational capacity, model size and amount of data can compensate for the other ~\citeyear[3]{scalinglaws}.
As a consequence the smaller models trained on Oscar and GerParCor should generally be able to achieve better scores than their evaluation indicates.

The impact of modifying the tokenization on a single \ac{pos} category in masked language learning must be close to none, given that the tasks do not explicitly focus on that category.
In other endeavors ~\citeauthor{BITE} used the Penn Treebank to model each inflection with designated tokens instead of analyzing the textual surface form, showing that an improvement of WordPiece tokenization is morphologically augmentable with \ac{POS} ~\citeyear{BITE}.
Their preceding use of morphological concepts in language learning is found in their earlier research through adversarial shifting of morphological paradigms to improve robustness of existing language models (\cite[2927]{adversarialmorphin}).
Implementing supporting processes for morphological modeling on an integral scale seems to be a condition for training impactful models.
This could be one explanation for the performance of the Wordmap based models but does not cover the standard models.

The classification task used to see if the model predictions are noteworthy or negligible was partly inconclusive.
As stated before the smaller models do not compare to the bbgc model.
Since this one shot evaluation made even bbgc perform worse than its other classification tasks (\textcite{germeval}; GNAD\footnote{https://tblock.github.io/10kGNAD/}) there may be an interfering factor in the task design.
The high number false positives may be attributed to imbalanced sampling (\cite[5--6]{brownlee2020imbalanced}).
This could be an intrinsic property of the dataset or happen during the modification of the sample labels.
On a side note the Wordmap models were designed to explicitly tokenize verbs with the aim of encoding an important core element of language, but the items in the Wikinews dataset featured news titles sometimes using elliptic phrasing and entirely dropping verbs.
To explain the uniform performance drop in all models additional constraints have to be taken into consideration.
The models are all masked language models, being a fine-tuned version of bbgc.
Instead of unfreezing all layers and retraining a more suitable approach would be to try transfer learning instead, preserving useful ggbc encoding layers.
To find out which of the structural modeling factors prevented the model from performing satisfyingly more training would have to be done, paying special attention to backpropagation, controlling the loss function in both fine-tuning and benchmark testing (ideally on runtime).

Considering the overall low performance it is surprising to see mwg5 performing at the given rate.
Whether this can be ascribed to the aforementioned sample imbalance or learning rate jitter is impossible to tell in retrospect, although the latter is rather unlikely in view of the other small model performances using the same small learning rate.
Positively the transfer of new subwords from the POS vocabulary to the BERT tokenizer vocabulary does not visibly impair performance.
All the input verbs come from sanitized data and GerParCor, and since mwo5 (fine-tuned on Oscar) was the best performing small model there is even an argument for saying it did not carry over a vocabulary bias.
Ultimately it is hard to classify the performance in this iteration of fine-tuning and testing.
Judging solely by the scores it should not be compared to recent methods like the original inspiration FLOTA (\cite{FLOTA}).\\
These observations allow answering the first \hyperlink{hyp1}{hypothesis}: the classification task demonstrates irregular scoring behavior for Wordmap models surpassing (mwo5) and also failing to reach (mwg5) comparable standard models msg5 and mso5.
No effect could be connected to the tokenization with Wordmap, the null hypothesis remains.

Concerning the Wordmap vocabulary generation it can be said that Wordmap shows effective extraction and analysis of segments in german verbs.
Its segmenting capabilities hold up to the FLOTA algorithm under the assumption that other lexical POS categories yield similar results.
This assumption is favored by the idea of morphological complexity given in \autoref{sec:target-languages}.
From a linguistic point of view there are a few reservations that should be addressed.
The Wordmap algorithm does not work on any grammatical framework and is unable to connect morphemes that would otherwise be analyzed as one, for example circumfixes.
This makes for lighter and easier implementation but outsources the connection of spatially separated morphemes and their functional exponence to the layers of the neural network (unlike e.g.\ \textcite{BITE}).
Recent language models deliver for languages with allomorphisms or multiple exponence\footnote{Brief summary on morphological exponence: https://home.uni-leipzig.de/jtrommer/netzwerk_antrag.pdf} despite not analyzing segmental dependencies at all and perform very well (this does not mean they cannot be improved by doing so).


Naturally there is still room for improvement regarding the maximization function, scope and especially testing.
As of now the segmentation algorithm behaves similar to FLOTA in that it discriminates subwords by length, among other features.
The lexical bias is mainly provided the by the stand-alone vocabulary generated beforehand.
This can most likely be improved by incorporating relative subword frequency into the maximization weight, following the example of successful tokenizers like \ac{wp} and \ac{bpe} do nowadays.
More ample implications pose questions as to how Wordmap would fare under circumstances of reanalysis, loaning, grammaticalization or lexicalization.
These processes usually take a long time and require many (linguistically) productive cycles, so that affected words will probably appear in corpora for a Wordmap generation algorithm to catch.
Another remark to be made in respect to character frequency.
The problem of identifying segments and discarding characters that do not belong to an adjacent morpheme is not trivial.
For Wordmap this poses the greatest challenge, leading to most of the unwanted fragments in both functional and lexemic vocabulary.
A suggestion is to do character sampling for retrieving (maybe even POS specific) character frequencies and using those as dampening for the Wordmap segment detection.
The detection itself is still binary and should be revised to detect segment counts that are in a range instead.
During development, a good indicator of segment boundaries where values around the mean, which almost are consistently in proximity of each other.
Other interesting operations with Wordmaps worth mentioning are derivatives.
In general, the first (mathematic) derivative of a word map seems to coincide with a segment boundary.
The second derivative can also mark morpheme boundaries, though this is just a presumption based on observations during plotting.
In most cases the resolution of the Wordmap is too volatile.
The application scope and testing cycle determine each other.
Examining Wordmap performance for more POS categories and languages would broaden its scope and probably reach their limit at the domain of incorporating languages, at which point the method needs fundamental expansion.
At the given the test scores are the only statistical measure provided that may be used to verify \hyperlink{hyp2}{hypothesis 2}, except for the test scores.
The previous discussion of these scores revealed that they cannot be used to draw reliable conclusions about the Wordmap tokenizer performance.
A qualitative assessment suggests that segmentations are less noisy for the intended \ac{pos} class.
It would be in good faith to say that the tokenization is improved with Wordmap.
H0 stands unfalsified.

