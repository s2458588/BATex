Method: tokenizer threshold.
Interestingly, there are many segment thresholds to choose from

there should be a filter to mitigate the natural distribution of common characters like s, n to be detected as  part of a morpheme

whole integrated system with overhead pos for all lexical categories
scaled with the degree of fusion and inflection that language has

algorithm doesnt see circumfixes like ge---t) but only affixes, this could be possible with better noise control and map interpolation

Characterizing the functional and lexical morphemes by their respective average length has had a positibe impact on segmentation, seem to have impacted the models performance very much.