in this section the whole methodoloy is covered. what do i use in this thesis, why do i use it and lastly, how?
make sure the why covers methodological implications. (vergiss nicht alle pakete als quelle im Anhang)

\section{Requirements}
\label{sec:requirements}
A series of tools will help to achieve lexicalized tokenization.
They will be explained in this chapter along with their methodological edge.

% EVTL. MACHINE LEARNING MODEL AUFSPLITTEN IN MODEL, TOKENIZER VOCABULARY?
\subsection{Machine Learning Model}
\label{subsec:mlm}

\ac{bert} is a language learning transformer model designed for \ac{nlp} tasks (\cite{ATTENTION}).
Upon release it achieved higher performance scores compared to previously used \ac{lstm} models (\cite{BERTHIGH1}).
Two main model characteristics can be observed for \ac{bert}.
Firstly, it is the first \ac{lm} to implement simultaneous attention heads, allowing for bidirectional reading.
The methodological implication of reading to the left and right of a token is to include more information about the language in single embeddings.
Secondly, \ac{bert} introduced the (at the time novel) \ac{mlm} method for training.
The method involves masking a specified amount (default 15\%) of random tokens in the input sequence.
Masked tokens are guessed by the model which can then update its weights according to success or failure.

The \ac{nlp} community has since developed \ac{bert} and adapted it to the needs of contemporary \ac{nlp} problems (roberta, germanbert, mbert \uppercase{citation}).
Its wide support, comparability and versatility make \ac{bert} the model of choice for this thesis.
Another notable feature in \uppercase{bert} is the implementation of the WordPiece tokenizer module (\uppercase{\href{https://huggingface.co/course/chapter6/6?fw=pt}{quelle?}}).
Default BERT WordPiece tokenization is predominantly heuristic by combining strings based on a precalculated score.
A variety of pre-trained tokenizers are available, although they come with a caveat.
Once a tokenizer is trained on a dataset it is specific to that dataset.
This means the application of a tokenizer on another dataset may result in out-of-vocabulary issues and different token/subtoken distributions.

Particularly relevant to this thesis is the option to train an own tokenizer from the base module.
Usually, WordPiece generates its own set of subtokens called \textit{vocabulary}.
Tokens are then \uppercase{WORDPIECE algorithmus erklären}
By providing an algorithmically generated vocabulary to WordPiece and then training it on a new dataset the tokenization behavior is changed.




\subsection{Data}
\label{subsec:data}
explain the data that is used

\subsection{Benchmark}
\label{subsec:benchmark}
explain olmpics

\section{Implementation}
\label{sec:implementation}

Tatsächliche Anwendung der Methoden auf die Daten

\subsection{Tokenizer}
\uppercase{essentially deriving sensible subtokens to represent lexemes}
\label{subsec:tokenizer-training}

\subsubsection{Generating a custom pre-training vocabulary}
\label{subsubsec:generating-a-custom-pre-training-vocabulary}

Algorithm for the pre-training vocabulary:
\begin{document}
    \begin{algorithm}
        \caption{Generate }\label{alg:cap}
        \begin{algorithmic}
            \Require $n \geq 0$
            \Ensure $y = x^n$
            \State $y \gets 1$
            \State $X \gets x$
            \State $N \gets n$
            \While{$N \neq 0$}
                \If{$N$ is even}
                    \State $X \gets X \times X$
                    \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
                \ElsIf{$N$ is odd}
                \State $y \gets y \times X$
                \State $N \gets N - 1$
                \EndIf
            \EndWhile
        \end{algorithmic}
    \end{algorithm}
    \subsubsection{Tokenizer Training}
    \label{subsubsec:tokenizer-training}
    How did I train the tokenizer, how did it go?
    Which problems arose? What went well?

    \subsection{Masked Language Model}
    \label{subsec:masked-language-model}
    Model implementation and parameters, runtimes?

    \subsection{oLMpics Benchmark}
    \label{subsec:olmpics-benchmark}



    tweak des tokenizers: segmentation ist eine frage der interpretation.
    The
    Ultimately, segmentation is a matter of interpretation.
    As mentioned in~\ref{subsec:mlm}, the default WordPiece Tokenizer lacks
    A linguistically informed



    The field of NLP  (\cite{METZLER2016}) has been expanded ever since the emergence of the language models.
    Natural language processing is understood as the