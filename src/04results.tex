\section{Benchmark}
\label{sec:benchmark}
This section is a comparative description of the results from the sequence classification task described in \autoref{subsec:benchmark-implementation}.
For every benchmark of three epochs precision, recall and F1 are given.
Precision reflects the amount of correct predictions a model has made for that particular class.
Recall shows how often the model correctly predicts a class in relation to all positive predictions.
F1 is a score compromising precision and recall through the \textit{harmonic mean} of precision and recall, measuring the models accuracy.

There is a common trend in all runs displaying a growth in precision, recall and F1.
This means that all models have continously improved predicting the class of a news title.
The highest F1 in the last epoch achieved for any of the smaller models (mso5, msg5, mwo5, mwg5) is mwo5 with a score of 0.69194 (\autoref{tab:mwo5}).
Meanwhile, the lowest F1 is found in \autoref{tab:mwg5} at 0.590542 mwg5.
A similar arrangement is found when comparing final test scores in the set of smaller models: mwo5 > mso5 > msg5 > mwg5 ($0.442827 > 0.405168 > 0.392490 > 0.389116$).

\begin{table}[H]
    \centering
    \begin{tabular}{lrrlr}
        \toprule
        \textbf{mwo5} & \textbf{Epoch 1} & \textbf{Epoch 2} & \textbf{Epoch 3} & \textbf{Test score} \\
        \midrule
        Precision & 0.292614 & 0.446338 & 0.71387 & 0.449735 \\
        Recall & 0.329531 & 0.552598 & 0.73384 & 0.474525 \\
        F1 & 0.242739 & 0.473851 & 0.69194 & 0.442827 \\
        \bottomrule
    \end{tabular}
    \caption[Metrics for model mwo5]{Metrics for masked language model trained on the Oscar dataset with Wordmap infused  tokenization. Evaluated on sequence classification task.}
    \label{tab:mwo5}
\end{table}

Of all small models, mwo5 has the best results for all three metrics precision, recall and F1.
It seemingly learns the fastest out of the four, starting with a recall of 0.329521  and ending on 0.73384 on the last epoch, overtaking its standard counterpart mso5 at the second epoch.

\begin{table}[H]
    \centering
    \begin{tabular}{lrrrr}
        \toprule
        \textbf{mwg5} & \textbf{Epoch 1} & \textbf{Epoch 2} & \textbf{Epoch 3} & \textbf{Test score} \\
        \midrule
        Precision & 0.237664 & 0.399781 & 0.603534 & 0.441891 \\
        Recall & 0.244613 & 0.463878 & 0.637516 & 0.440304 \\
        F1 & 0.163024 & 0.389905 & 0.590542 & 0.389116 \\
        \bottomrule
    \end{tabular}
    \caption[Metrics for model mwg5]{Metrics for masked language model trained on the GerParCor dataset with Wordmap infused tokenization. Evaluated on sequence classification task.}
    \label{tab:mwg5}
\end{table}

Contrary to the pattern of mwo5, the wordmap model trained on GerParCor data predicts less accurately than the standard msg5 model.
The inital epoch shows the lowest scores of all models for precision, recall and logically F1.
None of the predictions keep up with the accuracy of the other small model at any point in training.

\begin{table}[H]
    \centering
    \begin{tabular}{lrrrr}
        \toprule
        \textbf{mso5} & \textbf{Epoch 1} & \textbf{Epoch 2} & \textbf{Epoch 3} & \textbf{Test scores} \\
        \midrule
        Precision & 0.269615 & 0.422096 & 0.596987 & 0.395879 \\
        Recall & 0.351077 & 0.501901 & 0.657795 & 0.446388 \\
        F1 & 0.266260 & 0.412598 & 0.604824 & 0.405168 \\
        \bottomrule
    \end{tabular}
    \caption[Metrics for model mso5]{Metrics for masked language model trained on the GerParCor dataset with \ac{bbgc} tokenization. Evaluated on sequence classification task.}
    \label{tab:mso5}
\end{table}

With an F1 of 0.604824 at the final epoch mso5 is the lowest scoring standard model, despite starting with similar scores in epoch 1 (\autoref{tab:mso5} vs. \autoref{tab:msg5}).
The standard F1 test scores are close to each other, deviating only by one second decimal point indicating comparable benchmark performance.
A comparison with the scores of the other Oscar-trained model mwo5 shows a bigger difference in accuracy, favoring the Wordmap version.
The msg5 iteration is has a slightly steeper adaptation to the task, but fails to carry its performance over to the test scores:

\begin{table}[H]
\centering
\begin{tabular}{lrrrr}
    \toprule
    \textbf{msg5} & \textbf{Epoch 1} & \textbf{Epoch 2} & \textbf{Epoch 3} & \textbf{Test scores} \\
    \midrule
    Precision & 0.297466 & 0.517110 & 0.656808 & 0.439873 \\
    Recall & 0.359949 & 0.544994 & 0.676806 & 0.439924 \\
    F1 & 0.267111 & 0.480420 & 0.626593 & 0.392490 \\
    \bottomrule
\end{tabular}
\caption[Metrics for model msg5]{Metrics for masked language model trained on the GerParCor dataset with \ac{bbgc} tokenization. Evaluated on sequence classification task.}
\label{tab:msg5}
\end{table}

As can be seen, msg5 converges more quickly than its oscar standard counterpart (\autoref{tab:msg5}).
Although it has better precision, recall and F1 throughout all epochs final its test F1 is minimally inferior to mso5.



\begin{table}[H]
    \centering
    \begin{tabular}{lrrrr}
        \toprule
        \textbf{bbgc} & \textbf{Epoch 1} & \textbf{Epoch 2} & \textbf{Epoch 3} & \textbf{Test scores} \\
        \midrule
        Precision & 0.646150 & 0.768675 & 0.860180 & 0.622436 \\
        Recall & 0.709759 & 0.804816 & 0.883397 & 0.637262 \\
        F1 & 0.660588 & 0.778371 & 0.868166 & 0.624789 \\
        \bottomrule
    \end{tabular}
    \caption[Metrics for model bbgc]{Metrics for masked language model baseline bert-base-german-cased\footnote{\href{https://huggingface.co/bert-base-german-cased}{bert-base-german-cased}}. Evaluated on sequence classification task.}
    \label{tab:bert-base-german-cased}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{lrrrrr}
        \toprule
        \textbf{Summary} & \textbf{bbgc} & \textbf{std+oscar} & \textbf{std+gpc} & \textbf{wmt+oscar} & \textbf{wmt+gpc} \\
        \midrule
        Precision & \textbf{0.622436} & 0.395879 & 0.439873 & 0.441891 & 0.449735 \\
        Recall & \textbf{0.637262} & 0.446388 & 0.439924 & 0.440304 & 0.474525 \\
        F1 & \textbf{0.624789} & 0.405168 & 0.392490 & 0.389116 & 0.442827 \\
        \bottomrule
    \end{tabular}
    \caption{Test score summary for all evaluated models.}
    \label{tab:test-summary}
\end{table}

All small model scores are relatively close to each other, but ultimately outperformed by the baseline model.


\section{Tokenization}
\label{sec:tokenization}
Show specific examples of tokenization and analyze them qualitatively (maybe quantitatively)


wordmap: range around the median but

